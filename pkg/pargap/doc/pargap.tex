%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%A  pargap.tex         Parallel GAP/MPI documentation        Gene Cooperman
%
%
%

\pretolerance=500 % Will tolerate badness of 500 before trying hyphenations
\tolerance=1600 % Will tolerate stretching line up to badness of 1600
\hbadness=4000 % Seems to affect overfull boxes reported by TeX
\hfuzz=5pt % If still no good break, can stick out into margin by 5 pt.
\overfullrule=0pt % Lines sticking out more than 10 pt should not
                  % contain the black box marking it.

\Chapter{Writing Parallel Programs in GAP Easily}

This first chapter is intended to help a new user set up
Parallel GAP/MPI and run through some quick examples.  The later
chapters present detailed explanations of the facilities of
Parallel GAP/MPI.  Parallel GAP/MPI is often called ParGAP
(or ParGAP/MPI) for short.  Because parallel programming is sufficiently
different from sequential programming, this author recommends printing
out at least Chapters~1 through~"Tutorial using TOP-C and MasterSlave()",
and skimming through  those chapters for areas of interest, before
returning to the terminal to try out some of the ideas.
This document can be found in `.../pkg/pargap/doc/manual.dvi'
of the software distribution.  You may also want to print the index
at the end of `manual.dvi'.  In particular, the heading `example'
in the index, or `??example' from within GAP, should be useful.
If you prefer postscript, the UNIX command `dvips' will convert
that file to postscript form.

The word <MPI> in ParGAP/MPI refers to <Message Passing Interface>, a
well-known standard for parallelism.  ParGAP/MPI is based on the <MPI>
standard, and this distribution includes a subset implementation of MPI,
to provide a portable layer with a high level interface to BSD sockets.
However, knowledge of MPI is not required for use of this software, and so
we will often simply refer to ParGAP.

The development of ParGAP/MPI was partially supported by 
National Science Foundation grants CCR-9509783 and CCR-9732330.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Section{Overview of Parallel GAP/MPI (ParGAP)}

ParGAP currently is functional only on UNIX installations.  (Cygwin for
Windows is also an option, if you would like to port it.)  ParGAP/MPI can
be installed on top of an existing GAP installation.  See~"Installing
ParGAP/MPI" for instructions on installation of ParGAP/MPI.  At the time
that ParGAP/MPI is invoked, a special procgroup file must be available to
tell ParGAP/MPI which processors to use for slave processors.  See sections
"Installing ParGAP/MPI" and "Extended Example"
for instructions on invoking ParGAP/MPI.  If there are questions or bugs
concerning ParGAP/MPI, please write to:  gene@ccs.neu.edu

If one wishes only to try out the parallel features, the first five
pages of this manual (through the section on the slave listener) will
suffice for installation, and using it.  For the more advanced user
who wishes to design new parallel algorithms or port old sequential
code to a parallel environment, it is strongly recommended to also
read the following sections beginning with: "Basic Concepts for
the TOP-C model (MasterSlave)".

ParGAP/MPI must be invoked through the provided script,
`<GAP_ROOT_DIR>/bin/pargap.sh', which invokes
`<GAP_ROOT_DIR>/bin/\*/pargapmpi'.  MPI and
the higher layers will not be available if the binary is invoked in the
standard way as gap.  This is a feature, since a single binary and source
distribution serves both for the standard GAP and for ParGAP/MPI.

ParGAP/MPI is implemented in three layers: 1)~MPI, 2)~Slave~Listener, and
3)~Master~Slave (TOP-C abstraction).  Most users will find that the two
highest layers (Slave Listener and Master Slave) meet all their needs.

\beginitems
`1) MPI:'&

`'&The lowest layer is MPI.  Most users can ignore this layer.  MPI is
   a standard for message-based parallel computation.  A subset of the
   original MPI commands is provided.  The syntax is modified from the
   original C binding to make a GAP binding in an interpreted
   environment more convenient.  This includes default arguments,
   useful return values, and Error break in the presence of errors.
   `MPI_Init()' and `MPI_Finalize()' are invoked
   automatically by ParGAP/MPI.

`'&The MPI layer is not documented, since most users will not be
   using it.  From GAP level, you can type: `MPI_<tab><tab>' to see all
   implemented MPI functions and variables.  However, typing the
   symbol name alone (e.g.: `MPI_Send;' ) will cause it to display the
   calling syntax.  The same information is displayed when after an
   incorrect call.  The return value is typically obvious.  MPI is
   implemented in `src/pargap.c'.  The standard distribution uses a
   simple, subset implementation of MPI in `pkg/gapmpi/mpinu/', which is
   implemented on top of a standard sockets interface.  It is possible
   to substitute other implementations of MPI.

\index{MPI!standard}

`'&For those who wish to directly use the MPI interface, the meanings
   of the MPI calls are best found from the standard MPI
   documentation:

`'&\begintt
   <a href="http://www.mpi-forum.org/"> MPI Forum</a>
   <a href="http://www.mpi-forum.org/
           docs/mpi-11-html/mpi-report.html">
      MPI Standard (version 1.1)</a>
   <a href="http://www-c.mcs.anl.gov/mpi/www/">UNIX style man pages</a>
\endtt

`2) Slave Listener:'&

`'&This layer provides basic message passing facilities for
   communication among multiple ParGAP/MPI processes in a form that is
   more convenient for programming than the lower MPI layer.  This
   will be the most useful entry point to ParGAP/MPI for most users.
   This is the default mode for ParGAP/MPI.  Each remote (slave) process
   is in a receive-eval-send loop, in which the slave receives a GAP
   command from the local or master, the slave evaluates the GAP
   command, and the slave then sends the result back to the master as
   a GAP object.

`'&Almost all commands in the slave listener are of the form
   `*Msg*' (e.g.: `SendMsg()', `RecvMsg()',
   `ProbeMsg()' ).  Since the slave is in a receive-eval-send
   loop, every `SendMsg()' on the master must be balanced by a
   later `RecvMsg()'. `SendRecvMsg()' is provided to combine
   these steps.  A few parallel utilities are also included, such as
   `ParRead()', `ParList()', `ParEval()', etc.

`'&Messages are arbitrary GAP objects.  Note that arguments to any GAP
   function are evaluated before being passed to the function.  Hence,
   any argument to `SendMsg()' or `ParEval()' would be
   evaluated locally before being sent across the network.  For this
   reason, arguments can also be given as strings, to delay evaluation
   until reaching the destination process.  Hence, real strings must
   be quoted: `ParEval("x:=\"abc\";");' Additionally, multiple
   commands are valid, and the final ```;''' of the string is
   optional.  So, one can write:

\begintt
       BroadcastMsg("x:=\"abc\"; Print(Length(x), \"\\n\")");;
\endtt

`'&A full description is contained in Chapter~"Slave Listener".

`3) Master Slave:'&

`'&The Master Slave facility is provided both for writing complex
   parallel software, and as an easier way to parallelize previous or
   ``legacy'' sequential code.  While the Slave Listener may be
   sufficient for simple parallel requirements, more complex software
   requires a higher level abstraction.  The fundamental abstractions
   of the master slave layer are the {\it task} and the {\it environment}.

\beginitems
`'&`1)'& The task typically corresponds to the procedure or inner body
       of a loop in a sequential program.  This is the part that must
       be repetitively computed in parallel.

`'&`2)'& The environment typically corresponds to the data of a
       sequential program that is not within the local scope of the
       task.  Often this is a global data structure.  In the case that
       the task is the inner body of a loop, the environment may be a
       local data structure that is outside the local scope of the
       loop.
\enditems

`'& It is usually quite easy to identify the task and the environment
    of a sequential program or algorithm, which is the first step in
    parallelizing an algorithm.

`'& The Master Slave parallel model described here has also been
    successfully used in~C and in LISP.  It has been used both in
    distributed memory and shared memory environments, although this
    version in GAP currently works only in a distributed environment.
    In the C~language, this parallel model is known as TOP-C (Task
    Oriented Parallel~C).

`'& While no parallel software can eliminate the problem of designing
    an algorithm that is efficient in a parallel environment, the
    TOP-C abstraction eases the job by eliminating programmer concerns
    about lower level details, such as message passing, migration and
    replication of data, load balancing, etc.  This leaves the
    programmer to concentrate on the primary goal: maximizing the
    concurrency or parallelism.
\enditems

%======================================================================
\Section{Installing ParGAP/MPI}

\index{installation}

Installing ParGAP/MPI should be relatively simple.  However, since
there are many interactions both with the GAP kernel and with the
UNIX operating system, in a minority of cases, manual intervention
will be necessary.  If you are part of this minority, please
see the section "Problems with Installation".  The most common
problem is the local security policy.  ParGAP/MPI is more pleasant to
use when you don't have to manually provide the password for each
slave.  See section "Problems with Passwords (Getting Around Security)"
for suggestions in this respect.

\begintt
  cd .../gap4r3/pkg/
  gunzip pargap.tar.gz
  tar xvf pargap.tar
\endtt

If you are running GAP-4.0 or later, ParGAP/MPI can be installed in
the same way as other GAP share package:
\begintt
  cd .../pkg/pargap
  ./configure ../..
  make
\endtt

If you are running GAP-4b5, you should
\begintt
cp Makefile.in-4b5 Makefile.in
\endtt
and then proceed as above.

You may also wish to examine the two files,
`../pkg/pargap/bin/pargap.sh' and \penalty-50000
`.../pkg/pargap/bin/procgroup',
to verify if the paths in those files are correct for your installation.

If you had trouble installing it, see the section~"Problems with Installation".
Otherwise try out ParGAP/MPI.

%======================================================================
\Section{Extended Example}

After installation, try it out.  The example commands here are also
found in the README file.  So, you may wish to copy text from
the README file and paste it into a ParGAP/MPI session.
If you are using the unmodified procgroup file, your *remote slaves*
will be other processes on your local machine.  It is a good idea
to run only on your local machine for your first experiments and
while you are debugging parallel programs.  When you wish to
experiment with using remote machines, you can then proceed to the
following section, "Invoking ParGAP/MPI with Remote Slaves".

\index{example!Slave Listener}\index{Slave Listener!example}

\beginexample
  cd .../pkg/pargap/bin
  ./pargap.sh

[ALTERNATIVE:  PATH/pargap.sh -p4pg PATH/procgroup]

gap>  # This assumes your procgroup file includes two slave processes.
gap>  PingSlave(1);
gap>  SendMsg( "Print(3+4)" );
gap>  SendMsg( "Print(3+4,\"\\n\")" );
gap>  SendMsg( "3+4", 2);
gap>  RecvMsg( 2 );
gap>  FlushAllMsgs();
gap>  SendRecvMsg( "Exec(\"pwd\")" );
gap>  SendMsg("for i in [1..1000000] do for j in [1..1000000] do od; od");
gap>  SendMsg("Print(\"WAKE UP\\n\")");
gap>  ProbeMsgNonBlocking();
gap>  ParReset();
gap>  FlushAllMsgs();
gap>  SendRecvMsg( "a:=45; 3+4", 1 );
gap>  SendMsg( "a", 2 );   # Note "a" defined only on slave 1, not slave 2
gap>  RecvMsg( 2 );
gap>  SendMsg( "a", 1 );
gap>  RecvMsg( 1 );
gap>  myfnc := function() return 42; end;
gap>  BroadcastMsg( PrintToString( "myfnc := ", myfnc ) );
gap>  SendRecvMsg( "myfnc()", 1 );
gap>  FlushAllMsgs();
gap>  squares := ParList( [1..100], x->x^2 );
gap>  MSexample();
gap>  BroadcastMsg( PrintToString("MSList := ", MSList) );
gap>  ParTrace := false;
gap>  BroadcastMsg( PrintToString("MSList := ", MSList) );
gap>  ParEval( "MSList( [10..20], x->x^2 )" );
gap>  ParRead( "/home/gene/.gaprc" );
gap>  ParInstallTOPCGlobalFunction( "MyParList",
function( list, fnc )
  local result, iter;
  result := [];
  iter := Iterator(list);
  MasterSlave( function() if IsDoneIterator(iter) then return NOTASK;
                          else return NextIterator(iter); fi; end,
               fnc,
               function(input,output) result[input] := output; 
                                      return NO_ACTION; end,
               Error
             );
  return result;
end );
gap> MyParList( [1..25], x->x^3 );
gap> ParInstallTOPCGlobalFunction( "MyParListWithAglom",
function( list, fnc, aglomCount )
  local result, iter;
  result := [];
  iter := Iterator(list);
  MasterSlave( function() if IsDoneIterator(iter) then return NOTASK;
                          else return NextIterator(iter); fi; end,
               fnc,
               function(input,output)
                 local i;
                 for i in [1..Length(input)] do
                   result[input[i]] := output[i];
                 od;
                 return NO_ACTION;
               end,
               Error,  # Never called, can specify anything
               aglomCount
             );
  return result;
end );
gap> MyParListWithAglom( [1..25], x->x^3, 4 );

\endexample

If you wish an accelerated introduction to the models of parallel
programming provided here, you might wish to read the beginning of
Chapter~"Slave Listener" through section~"Slave Listener Commands", and
then proceed immediately to Chapter~"Basic Concepts for the TOP-C model
(MasterSlave)".

The ParGAP/MPI
share package was designed and written by Gene Cooperman,
College of Computer Science, Northeastern University, Boston, MA, U.S.A.

If you use ParGAP/MPI to solve a problem then please send a short email
to `gene@ccs.neu.edu' about it, and reference the ParGAP/MPI package
as follows:

\begintt
\bibitem[Coo99]{Coo99}
      Cooperman, Gene,
      {\sl Parallel GAP/MPI (ParGAP/MPI)}, Version 1,
      College of Computer Science, Northeastern University, 1999,
      \verb+http://www.ccs.neu.edu/home/gene/pargap.html+.
\endtt

%======================================================================
\Section{Invoking ParGAP/MPI with Remote Slaves}

ParGAP/MPI, unlike GAP, must be invoked under a separate name.  After
GAP/MPI has been installed, a script `GAP_ROOT_DIR/bin/pargap.sh' will
then invoke GAP.  This is similar to `GAP_ROOT_DIR/bin/gap.sh' that is
used to invoke GAP.  Installers are encouraged to treat gapmpi.sh in
analogy to gap.sh.  For example, if your site has copied gap.sh to
/usr/local/gap, then you should also look for the gapmpi.sh script as
/usr/local/gapmpi.

In addition, when gapmpi is called, there must be a file, procgroup,
in the current directory.  Alternatively, if you wish to use a single
procgroup file for all jobs, and that procgroup file is in /home/joe,
then you can alias gapmpi to ``gapmpi -p4pg /home/joe/procgroup''.

The procgroup file has a simple syntax, taken from the MPICH
implementation of MPI (inherited from P4).  A '\#' in column~1
introduces a comment line.  The first non-comment should be ``local 0'',
verbatim.  This line declares the master process as the local process.
Other lines are of the form:

\begintt
regulus.ccs.neu.edu 1 /usr/local/bin/gapmpi.sh
\endtt

The first field is the hostname for a remote process.  The second
field specifies one thread per process.  (ParGAP/MPI recognizes only the
value~1 for the second field.)  The third field is an absolute pathname
for ParGAP/MPI, as it would be called on the remote process.  Note that you can
repeat the same line twice if you want two remote ParGAP/MPI processes on the
same processor.  The default procgroup provided in the distribution
uses:

\begintt
localhost 1 gapmpi
\endtt

This will work only if gapmpi is in your path on the remote machine
shell (localhost in this case), using your default shell.  On most
machines, localhost is an alias for the local processor.  This is a
good default for debugging, so that you don't disturb users on other
machines.

MPI will use such a line to create a UNIX subprocess executing:

\begintt
   rsh regulus.ccs.neu.edu /usr/local/bin/gapmpi.sh
\endtt

In the above example, if you have trouble invoking ParGAP/MPI, it is a
good idea to first invoke ``rsh regulus.ccs.neu.edu'' from a UNIX
prompt.  If that succeeds, try executing the full ``rsh'' command line
above.

A typical problem is that the remote processor requires a password to
login.  MPI requires a login without passwords.  A typical problem is
that /etc/hosts.equiv has not been set up to remove the password
requirement for your remote host.  Sometimes this can be solved by an
appropriate .rhosts file in your home directory on the remote host.
Sometimes, PAM is also used for user authentication (see
/etc/pam.conf).  ``man in.rshd'' also has helpful information.  Consult
your system staff for further analysis.

Note that the remote ParGAP/MPI process will not read from standard
input, although signals such as SIGINT (\^C) may be received by the
remote process.  However, the remote ParGAP/MPI process will write to
standard output, which is relayed to the local process.  So,

\begintt
        SendMsg("Exec(\"hostname\")", 2);
\endtt

will execute and print from the remote process.

%======================================================================
\Section{Problems with Installation}

If you still have problems, here is a list of things to check.

\beginitems
1.& Do you have enough swap space to support multiple GAP processes?
    A simple way to check this is with the UNIX command, `top'.
    The Linux version of `top' sorts by memory usage if you type `M'.

2.& `make' tries to automatically create
     `pkg/pargap/bin/pargap.sh', and copy the
     parameters\penalty-1000 from `<GAP_ROOT>/bin/gap.sh'.
     <GAP_ROOT> was specified when you executed `./configure
     <GAP_ROOT>' to install ParGAP/MPI.
     This can be error-prone if your site has an unusual setup.
     If you execute `<GAP_ROOT>/bin/gap.sh', does gap come up?
     If so, compare it with pargap.sh and check for
     correct settings in `.../pkg/pargap/bin/pargap.sh'?

3.& Did ParGAP find your procgroup?
     [ It looks in the current directory, or for:
\begintt
          ... -p4pg PATH/procgroup
\endtt
   &    on the command line. ]

4.& Were the remote slave processes able to start up?  If so, could they
       connect back to the master?
       To test connectivity problems,
       try manually starting a remote slave by executing a line in the
       script.  Try a simple `rsh remote_hostname' to see if the issue
       is with security.
       If your site uses `ssh' instead of `rsh', then there is a security
       issue.  Read~"Problems with Passwords (Getting Around Security)",
       and possibly `man sshd'.

5.& It the previous step failed due to security issues, such as requesting
       a password, you have several options.  `man rshd' tells you the
       security model at your site (or possibly `man ssh' if you use that).
       Then read~"Problems with Passwords (Getting Around Security)".

6.& Is the procgroup file in your current directory set correctly?
     Test it.  If you are calling it on a remote host, manually type:
\begintt
       rsh <HOSTNAME> <BINARY>
\endtt
   & where <HOSTNAME> and <BINARY> appear exactly as in procgroup.
     For example:  `rsh denali.ccs.neu.edu /usr/local/gap4r3/bin/pargap.sh'.
     In some cases, `exec' is used to save process overhead.  Also try:
\begintt
       rsh <HOSTNAME> exec <BINARY>
\endtt
   & If you plan to call it on localhost, try just:   <BINARY>

   & Note that if not all the slave processes succeed in connecting
       to the master, then Parallel GAP writes out a file,
       `/tmp/pargapmpi--rsh.\$\$', where \$\$ is replaced by the the
       process id of Parallel ParGAP/MPI.

7.& Is `pargap' listed in `.../pkg/ALLPKG'?
     [ It's needed to autostart slaves.]

8.& Inside Parallel GAP, has MPI been successfully initialized?
     Try:  `MPI_Initialized();'

9.& A remote (slave) ParGAP process starts in your home directory
     and tries to cd to a directory of the same name as your local directory.
     Check your assumptions about the remote machine.  Try:
       `SendRecvMsg("Exec(pwd)"); SendRecvMsg("UNIX_Hostname()");
       SendRecvMsg("UNIX_Getpid()");'

10.& If the connection dies at random, after some period of time:
       You can experiment with SO_KEEPALIVE and variants.  (man setsockopt)
       This periodically sends *null messages* so the remote machine
       does not think that the originating machine is dead.
       However, if the remote machine fails to reply, the local process
       sends a SIGPIPE signal to notify current processes of a broken socket,
       even though there might have been only a temporary lapse in
       connectivity.
       ssh specifies `KeepAlive yes' by default, but setting `KeepAlive no'
       might get you through some transient lapses in connectivity due
       to high congestion.
       You may also want to experiment with:  `setenv RSH "rsh -n"'

11.& Read the documentation for further possible problems.
\enditems

%======================================================================
\Section{Problems with Hosts on Multiple Networks}

If a host is on multiple networks, it will have multiple IP addresses
and usually multiple hostnames.  In this case, the master process cannot
always guess correctly which IP address (which internet address) should be
passed to the slave process, so that the slave process can call back to the
master.  In such cases, you may need to tell ParGAP/MPI which hostname
or IP address to use for the callback.  This is done by setting the
UNIX environment variable, `CALLBACK_HOST', as in the example below.
\begintt
# [ in sh/bash/... ]
CALLBACK_HOST=denali.ccs.neu.edu; export CALLBACK_HOST
# [ in csh/tcsh/... ]
setenv CALLBACK_HOST=denali.ccs.neu.edu
\endtt

The appropriate line for your shell can be placed in your shell initialization
file.  Alternatively, you can set this up for all users by placing
the Bourne shell version (for `sh') somewhere between the first and last line
of `.../pkg/pargap/bin/pargap.sh'.

%======================================================================
\Section{Problems with Passwords (Getting Around Security)}

There is a simple test to see if you need to read this section.
Pick a remote machine, <HOSTNAME>, that you wish to execute on,
and type:  `rsh <HOSTNAME>'.  If this did not work, also try
`ssh <HOSTNAME>'.  If you were asked for your password, then you and
your system administrator may need to talk about security policy.
If you were successful with `ssh' and not with `rsh' then set
the environment variable, `RSH', to the value `ssh', as described in
item~3 below.

\beginitems
     (1)& Ask your systems administrator to put the machines in a hosts.equiv
         file, so that logging in from one to the other does not require
         a password.  (man hosts.equiv)

     (2)& Add a .rhosts file to your home directory (or .shosts for ssh).

     (3)& Hack around the problem:
         By default, the startup script uses `rsh' to start remote
         processes.  However, if the environment variable RSH was set,
         the script uses the value of the environment variable instead of `rsh'.
         This may be useful, if you have your own script, `myrsh', that
         automatically gets around the security issues.  Then just type:
         \hfil\break
         `RSH=myrsh; export RSH'   [ in sh/bash/... ]
         \hfil\break
         `setenv RSH myrsh'        [ in csh/tcsh/... ]
         \hfil\break
         The appropriate line for your shell can be placed in your shell
         initialization file.  Alternatively, you can set this up for all
         users by placing the Bourne shell version (for `sh') somewhere
         between the first and last line of
         `.../pkg/pargap/bin/pargap.sh'.

     (4)& ssh:  `man ssh' mentions some possibilities for giving the password
         the first time, and then having ssh remember that future logins
         to that machine are authorized for the duration of the session.
         Don't overlook the use of `\$HOME/.ssh/config' to set special
         parameters, such as specifying a different login name on the
         remote machine.  Some parameters of interest might be KeepAlive,
         RSAAuthentication, UseRsh.  You may also find useful information
         in `man sshd'.

     (5)& After starting Parallel ParGAP/MPI, manually call
           `/tmp/pargapmpi--rsh.\$\$'
         and repeatedly type in the password for each slave process.
         If you find yourself doing this, you may want to talk with
         your system administrator, since it actually hurts system
         security to have you repeatedly typing passwords with a
         concommitant risk that someone else will find out your password.
\enditems

%======================================================================
\Section{Modifying GAP kernel}

Note that this package modifies the GAP src and bin files, and creates
a new GAP kernel.  This new GAP kernel can be shared by traditional users
of the old, sequential GAP kernel, and by those doing parallel processing.

The GAP kernel will have identical behavior to the
old GAP kernel when invoked through the gap.sh script or the
`bin/@GAParch@/gap' binary.  The new ParGAP/MPI variables will appear to the
end user *ONLY* if the GAP binary was invoked as pargapmpi:  a symbolic
link to the actual GAP binary.  The script, pargap.sh, does this.

So, in a multi-user environment, traditional users can continue to use
gap.sh without noticing any difference.  Only an invocation as pargap.sh
will add the new features.

In a future version of GAP, it is hoped
that the GAP kernel will have enough ``hooks'', so that no
modification of the GAP kernel is required.  At that time, it will
also be possible to speed up the startup time for ParGAP/MPI.  Much of
the startup time is caused by waiting for GAP to read its library
files.  It will be possible to use the GAP function, `SaveWorkspace()'
to save a version with the GAP library pre-loaded.  That saved version
can then be used to start up ParGAP/MPI.  This is not currently
possible, because ParGAP/MPI needs to get at the command line of GAP
before the GAP kernel sees it.

Comments and contributions to a ParGAP/MPI user library, or any other type
of assistance, are gratefully accepted.

                                                        Gene Cooperman
                                                        gene@ccs.neu.edu

%======================================================================

\Chapter{Slave Listener}

ParGAP/MPI implements a model of a slave process as a *slave listener*.
This means that the slave is running a simple program:
\begintt
  (1) Read message from master [as string]
  (2) Evaluate message and return result
  (3) Send message to master with result [as string]
  (4) Goto step 1
\endtt
An example using this interactive style is contained in
section~"Extended Example".

Some enhancements to this simple model should also be noted.  The reply
message from the slave will wait in a queue until the master process
decides to read it.  If unwanted messages accumulate in the queue,
the master can execute `FlushAllMsgs()'.  If a slave process prints
to the standard output, this will be visible at the console of the
master process.  If a slave process executes an error and goes into a
break loop, then it will automatically return to the top level, return
an error message to the master process, and wait for another message
from the master process.  If a slave process goes into an infinite loop,
the master process can call ParReset() to interrupt all slave processes
and return them to their top level loop as a slave listener.

At this point, you may wish to review the commands through the
extended example in section "Extended Example".  Note also some
naming conventions:
\beginitems
+& `MPI_...':  refers to a GAP binding of an MPI function.  These
functions are low level functions on which the rest of ParGAP/MPI is
built.  They can be safely ignored by the casual user.  (MPI, *Message
Passing Interface*, is a standard for message passing.)  In
GAP, type `MPI_'<TAB> for a list of all such functions.

+& `UNIX_...':  These are additional system commands that were not
present in the GAP kernel.  They are typically GAP versions of UNIX
commands that make life easier.  `UNIX_Nice()' is an example.  In
GAP, type `UNIX_'<TAB> for a list of all such functions.

+& `Par...':  refers to a function that should be called only on the
master process.  It invokes all slave processes to do its work.  In
GAP, type `Par'<TAB> for a list of all such functions.
\enditems

%======================================================================
\Section{Slave Listener Commands}

The implementation is in `pkg/pargap/lib/slavelist.g'.  Most
procedures are short, and can also be read online with commands such
as: `Print(SendMsg)'.  You may also find `Print(SlaveListener)' and
`Print(CloseSlaveListener)' useful in order to better understand the
behavior of the slave listener.  Examples of these commands can be
found in context in the section "Extended Example".  Some of these
commands are based on MPI.  Further information on basic concepts of
MPI can be found in section~"Tutorial introduction to the MPI C
library", but that section can be safely ignored on a first reading.

\>SendMsg( <command>[, <dest>[, <tag>]] ) F

The default value of <dest> is 1 if `IsMaster()', and <dest> is 0 (rank
of master) if `not IsMaster()'.  The default value of <tag> is 1.
Tags of value 1000 and above are reserved to use by ParGAP/MPI itself,
and should not be used by application routines.

\>RecvMsg( [<source>] ) F

The default value of <source> is `MPI_ANY_SOURCE', which receives the
next available message from any source.  `GetLastMsgSource()' allows
one to determine the source in such cases.  `GetLastMsgTag()' always
allows one to determine the tag, although most applications can ignore
the tag.

\>GetLastMsgSource() F

  Returns source of last message that was either received or simply probed.

\>GetLastMsgTag() F

  Returns tag of last message that was either received or simply probed.

\>SendRecvMsg( <command>[, <dest>[, <tag>]] ) F

  Equivalent to `SendMsg( <command>[, <dest>[, <tag>]] ); RecvMsg()'.
  Note especially that tag values of 1000 and higher are reserved for
  use by ParGAP/MPI.

\>BroadcastMsg( <command> ) F

   Executes <command> on slaves only.  SlaveListeners do not send back
   a return value.  Note that this use of the term *broadcast* is
   distinct from the MPI usage.  In MPI, a broadcast message will be
   received by every process, including the process sending the message.

\>IsMaster() F

Boolean:  true if at console (if `MPI_Comm_rank() = 0').

\>FlushAllMsgs() F

\>PingSlave()

\>ParEval( <stringCmd> ) F

Evaluate on all processes
   [like BroadcastMsg(), but ParEval() also executes on master
    and also returns a value based on result on master]

\>PrintToString( <object> [, ...] )

   [Note that `PrintToString("abc") => "abc"' 
    (like `Print()', NOT `"\"abc\""')
     Hence, a useful idiom is:  `ParEval( PrintToString( "foo := ", foo ) );'  ]

\>ParRead( <filename> ) F
\>ParReread( <filename> ) F

Like the GAP functions.  Read file on all processes.
Note that it is redundant (and often incorrect) to call `ParRead' on
a file that itself contains `Par...' functions.  One should either
place sequential functions in a file and call `ParRead' or place
`Par...' functions in a file and call `Read' from the master.
As an example, in writing this code, I found it useful to edit
`../lib/masslave.g' and then type `ParReread("../lib/masslave.g")'.

\>ParList( <fnc>, <list> ) F

Like the GAP function.  But faster since it also uses the slave processes.

\>ProbeMsg( [<source>] ) F

Probe for pending message from <source>.  It will block untile such
a message appears, and then return true.  ^C (interrupt) works to unblock it.
The default value of <source> is `MPI_ANY_SOURCE', which probes for
a message from any source.

\>ProbeMsgNonBlocking( [<source>] ) F

Exactly like `ProbeMsg', but non-blocking.  It returns immediately
with true or false, depending on whether a message was present
from <source>.  The default value of <source> is `MPI_ANY_SOURCE'.

\>ParBindGlobal( <gvar>, <value> ) F

Not currently implemented, due to certain technical considerations.

\>ParDeclareGlobalValue( <string> ) F
\>ParDeclareGlobalFunction( <string> ) F

Similar to corresponding GAP functions.
Note that unlike GAP\pif s `DeclareGlobalFunction' and `ParDeclareGlobalValue',
these functions also allow you to re-declare an old function or variable.
The net effect is to remove the old value, and allow one to again call
`InstallGlobalFunction' and `InstallValue'.  This eliminates the necessity
for Reread() in ParGAP/MPI, and it also makes it easier to place the commands
in a local file, and using a simple `Read()' instead of `ParRead()'.  It
also makes it easier to interactively re-declare and re-install
functions.

\>ParInstallValue( <gvar>, <value> ) F
\>ParInstallValue( <string>, <value> ) F
\>ParInstallGlobalFunction( <gvar>, <function> ) F
\>ParInstallGlobalFunction( <string>, <function> ) F

Note that the second version (with <string>) is equivalent to
\begintt
ParDeclareGlobalFunction( <string> )
ParInstallGlobalFunction( <gvar>, <function> )
\endtt
and similarly for `ParInstallValue',
where <gvar> is a GAP variable whose name is <string>.
Note that `ParInstallValue' is currently implemented only in the
version for <string>, due to certain technical considerations.

This completes the middle layer of ParGAP/MPI.  It allows one to easily
use parallelism interactively.  There are now two choices for further reading.
The recommended choice for writing your own parallel applications is to
read the next chapter on the TOP-C task-oriented model of parallelism, and
the follow-on chapter, containing a tutorial on the TOP-C model.
These two chapters should provide enough background to write
significant parallel applications.  If on the other hand you are
interested in MPI and the low-level fundamentals of message passing
for parallel applications, then you should read Chapter~"MPI commands
and UNIX systems calls in ParGAP/MPI".

%======================================================================
\Chapter{Basic Concepts for the TOP-C model (MasterSlave)}

TOP-C stands for *Task-Oriented Parallel C*~\cite{Coo96}.  The {\it
TOP-C model} is the specific master slave model implemented here.
That model has been adapted for use in ParGAP/MPI.  The implementation
is in `pkg/pargap/lib/masslave.g'.  Note that the functions and
variables with names `TOPC'... are intended as internal functions only,
and should not be used by the TOPC programmer.

For the impatient, you may type `MSexample()' in a ParGAP/MPI session
now.  If you prefer further hands-on learning in a tutorial style, you
may wish to next read the section, "Tutorial using TOP-C and
MasterSlave()".  Eventually, if you wish a deeper understanding of the
TOP-C model, you will need to read this current section and those that
follow.

The initial GAP process is the master process, and all others are {\it
slave} processes.  It allows most of the CPU-intensive computations to
be carried out on slave processes, which typically reside on remote
processors.  A well-developed TOP-C application should find that the
master process is almost never busy when a slave process is idle,
waiting for a new computation to carry out.  This provides a natural
way of maximizing utilization and load balancing.

The TOP-C model depends on three concepts:

\beginitems
1)& the <task>: a function that takes an arbitrary object as
              its single argument, reads some or all of the global
              environment, and then returns an arbitrary object as its
              value.  The task typically corresponds to the inner loop
              of a typical application.

2)& the <environment>: global data, shared among all processes.  This
        data can be read as part of the computation of a task.
        However, after initialization of the environment, this data
        must be written (modified) *only* by a particular application
        routine, `UpdateSharedData()'.

3)& the <action>:  After the output of a task has been produced, an
            application routine must choose one of four actions to
            determine how the output is used.
\enditems

The {\it task input} is defined to be the argument of the {\it task}
(considered as a function), and the {\it task output} is the return
value of the {\it task}.

%======================================================================
\Section{Basic TOP-C (Master-Slave) commands}

\index{TOP-C model}
\index{master slave model}

There is only one core TOP-C command, a utility function, and several
constants.  A TOP-C command must be evaluated on the master and on all
slaves.  We shall describe the commands in detail in the following
sections, but a short list of the essentials and a small example
will be helpful to set the context.

\>MasterSlave( <SubmitTaskInput>, <DoTask>[, CheckTaskResult[, <UpdateSharedData>[,
               <taskAgglomCount>]]] ) F

\>NOTASK V

\index{actions}

{\catcode 95=12
\>NO_ACTION V
\>UPDATE_ACTION V
\>REDO_ACTION V
\>CONTINUATION_ACTION() F
\par}

\>IsUpToDate() F

\>ParInstallTOPCGlobalFunction( <string>, <function> ) F
\>ParInstallTOPCGlobalFunction( <gvar>, <function> ) F

A short example shows one possible implementation of `ParList()'.

\beginexample
ParInstallTOPCGlobalFunction( "MyParListWithAgglom",
function( list, fnc )
  local result, i;
  result := []; i := 0;
  MasterSlave( function() if i >= Length(list) then return NOTASK;
                          else i := i+1; return i; fi; end,
               fnc,
               function(input,output) result[input] := output;
                                      return NO_ACTION; end,
               Error
             );
  return result;
end );
\endexample

%======================================================================
\Section{Other TOP-C Commands}

A master slave computation is invoked when a GAP program issues the
command `MasterSlave()'.  The typical form is:

\begintt
MasterSlave( <SubmitTaskInput>, <DoTask>
             [, <CheckTaskResult>[, <UpdateSharedData>[, <taskAgglom>]]] )
\endtt

The function, `MasterSlave()', is defined by ParGAP/MPI.  The first
four arguments to `MasterSlave()' are also functions, but they must be
defined by the application writer.  Their calling syntax is defined by
the following GAP code, which also provides a simplified description
of how a sequential (non-parallel) `MasterSlave()' would invoke these
functions if there were only a single process.  (A more sophisticated
version of this routine is provided in ParGAP/MPI to allow one to
debug within a single process first.)  The use of the fifth argument,
<taskAgglom>, is deferred until section~"Agglomerating tasks for
efficiency (ParSemiEchelonMat revisited again)".

In this section, we define `MasterSlave()' and describe the use of
its four arguments in a purely sequential environment.  The issues of
parallelism and passing of messages between processes is covered in
the next section.  The call to `MasterSlave()' in ParGAP/MPI, above,
will have the same result as if `MasterSlave()' were defined
equivalently to `SeqMasterSlave()' below, and then run in a
standard, sequential GAP (a single process).  The next section
describes the multi-process implementation of `MasterSlave()' in
ParGAP/MPI, in which `taskInput' is computed on the master process
and sent as a message to a slave process, while `taskOutput' is
computed on a slave process and sent as a message to the master
process.

\index{SeqMasterSlave!simplified pseudo-code}

\begintt
SeqMasterSlave :=
  function(SubmitTaskInput, DoTask, CheckTaskResult, UpdateSharedData)
  local taskInput, taskOutput, action;
  while true do
    taskInput := SubmitTaskInput();
    if taskInput = NOTASK then break; fi;
    repeat
      taskOutput := DoTask( taskInput );
      action := CheckTaskResult( taskOutput, taskInput );
    until action <> REDO_ACTION;
    if action = UPDATE_ACTION then
      # Modify the environment (global, shared data structures) here
      # Called on all processes, master and slaves
      UpdateSharedData( taskOutput, taskInput );
    fi;
  od;
end;
\endtt

One can also follow the life a single task in a multi-processing environment
through the diagram above.

`[ NO PICTURE ENVIRONMENT AVAILABLE ]'
% {\footnotesize
% \begin{figure}[htb]\label{topc-fig}
% \begin{center}\label{diagram}
% \setlength{\unitlength}{3.0pt}
% % \setlength{\unitlength}{2pt} % for 2-column mode
% \begin{picture}(100,95)
% \put(25,90){\makebox(0,0)[b]{\bf MASTER}}
% \put(75,90){\makebox(0,0)[b]{\bf SLAVE}}
% \put(50,00){\line(0,1){4}}
% \multiput(50,15)(0,5){16}{\line(0,1){4}} % 20 times, delta = (0,5)
% \thicklines
% \put(5,88){\line(1,0){95}}
% \put(25,75){\oval(20,10)}  % corner is (35,70)
% \put(25,75){\makebox(0,0){`SubmitTaskInput()'}}
% \put(75,55){\oval(25,10)} % corner is (62,5, 50)
% \put(75,55){\makebox(0,0){`DoTask(taskInput)'}}
% \put(25,32){\oval(45,10)} % corner is (47.5, 26)
% \put(25,32){\makebox(0,0){\parbox[t]{90pt}{`CheckTaskResult' \hbox{\ \ }`(taskOutput,~taskInput)'}}}
% \put(50,10){\oval(75,10)}
% \put(50,10){\makebox(0,0){`UpdateSharedData(taskOutput, taskInput)'}}
% \thinlines
% % text is near beginning of vector
% % ref. point is one corner of oval
% \thicklines
% \put(34,69){\vector(3,-1){27}} % base is length 20
% \put(36,69){\makebox(0,0)[bl]{`taskInput'}}
% \put(62,51){\vector(-3,-2){18}} % base is length 20
% \put(60,51){\makebox(0,0)[br]{`taskOutput'}}
% \thinlines
% \put(48,37){\vector(3,2){15}} % base is length 20
% \put(48,37){\makebox(0,0)[tl]{(if action == {\tt REDO\_ACTION})}}
% \put(25,25){\vector(3,-1){25}} % base is length 20
% \put(36,22){\makebox(0,0)[bl]{(if action == {\tt UPDATE\_ACTION})}}
% \end{picture}\break
%   \hbox{\large TOP-C Programmer's Model}
% \end{center}
% \end{figure}
% }


Although not explicit in the code, the application writer should add
comments to define what is the environment.  The environment is
defined as a global, shared data structure that is treated as
``read-write'' by `UpdateSharedData()', while being treated as
``read-only'' by `SubmitTaskInput()', `DoTask()', and
`CheckTaskResult()'. Note also that an application
writer may use different names for the four functions
`SubmitTaskInput()', etc.  It is only a convention within this
manual to give those functions the names, above.  Similarly,
`taskInput', `taskOutput' and `action' are the
conventional names used in this manual, and a given application may
use different names.

In a correct ParGAP/MPI application, the environment should be
initialized to the same value on all processes before the application
calls `MasterSlave()'.
`MasterSlave()' is then called on all processes.
After that, the environment can be modified only by a call to
`UpdateSharedData()', and `MasterSlave()' arranges for each call to
`UpdateSharedData()' to be executed on all processes.  Further,
`UpdateSharedData()' has access only to `taskInput',
`taskOutput', and the previous value of the environment.
Thus, `MasterSlave()' maintains the same environment uniformly on
all processes.

%======================================================================
\Section{Simple Usage of MasterSlave() }

This section is concerned with formal definitions for the routines
associated with ParGAP/MPI.  It is important to keep in mind the
pseudo-code of the section, "Basic Concepts for the TOP-C model
(MasterSlave)".  Since `MasterSlave()' uses all the ParGAP/MPI
processes, the user must invoke it on all processes.  This is
typically done through some function provided by the slave listener
layer, such as `ParEval()'. It may be instructive for the reader to
run ParGAP/MPI and type `MSexample()' now, or else to look at some
examples of ParGAP/MPI applications in the section~"Tutorial using
TOP-C and MasterSlave()".  This demonstrates the use of `MasterSlave()'
in a typical session.

The four functions written by the application writer are:
`SubmitTaskInput()', `DoTask()', `CheckTaskResult()',
and `UpdateSharedData()'.
`DoTask()' is executed on a slave.  `SubmitTaskInput()'
and `CheckTaskResult()' are executed on the master, where a
`taskInput' is generated and a corresponding `taskOutput' is
received.  Finally, `UpdateSharedData()' is executed on all
processes.  ParGAP/MPI arranges to automatically pass `taskInput' and
`taskOutput' between the master and a slave.

Since the single master process is responsible for generating all
`taskInput''s and receiving all `taskOutput''s, it is critical that
computation on the master process should not become a bottleneck for a
well-designed ParGAP/MPI application.  Accordingly, the application
writer should arrange for `SubmitTaskInput()' and
`CheckTaskResult()' to
execute quickly, even if this means additional computation by
`DoTask()' or `UpdateSharedData()'.

As seen in the examples, `SubmitTaskInput()' may use global variables on
the master to ``remember'' the last `taskInput' or other state
information.  Note that such global variables cannot be part of the
environment, since they are modified outside of `UpdateSharedData()'.

%======================================================================
\Section{Efficient Parallelism in MasterSlave() using CheckTaskResult()}

It is instructive to review the logic for the lifetime of a task, as
described by the pseudo-code in the section, "Basic Concepts
for the TOP-C model (MasterSlave)".  Initially, `MasterSlave()'
calls `SubmitTaskInput()' on the
master, which returns an application-defined GAP object, `taskInput'.
`MasterSlave()' then copies `taskInput' to an arbitrary slave
process, and `MasterSlave()' then calls
`DoTask( taskInput )' on the slave.  This returns an
application-defined GAP object,
`taskOutput', which
`MasterSlave()' copies to the master process.  On the master,
`MasterSlave()' then calls
`CheckTaskResult( taskInput, taskOutput )',
which returns an action.  (Recall that `taskInput', `taskOutput' and
`CheckTaskResult()' are defined by the application writer,
and so an application program may give them different names.)

There are four possible {\it actions} (GAP constants): `NO_ACTION',
`UPDATE_ACTION', `REDO_ACTION',
`CONTINUATION_ACTION( taskContinuation )'.
A standard language idiom in ParGAP/MPI is to define
`CheckTaskResult()' as the ParGAP function `DefaultCheckTaskResult()'.

\begintt
DefaultCheckTaskResult := function( taskOutput, taskInput )
  if taskOutput = false then return NO_ACTION;
  elif not IsUpToDate() then return REDO_ACTION;
  else return UPDATE_ACTION;
  fi;
end;
\endtt

{\catcode 95=12
\index{NO_ACTION!definition}
\par}

In the simplest case, `CheckTaskResult()' returns `NO_ACTION', in which
case there is no further computation related to the original
`taskInput'.  `CheckTaskResult()' may record global information on the
master process, based on the `taskOutput', but the environment, and
hence the state of the slave processes, will not be modified.

{\catcode 95=12
\index{UPDATE_ACTION!definition}
\par}

In the second most common case, `CheckTaskResult()'
returns `UPDATE_ACTION'. This action causes `MasterSlave()'
to call
`UpdateSharedData( taskOutput, taskInput )'
on all processes (master and slaves).  This is the {\it only} way in
which the environment can be modified by a correct ParGAP/MPI program.

{\catcode 95=12
\index{REDO_ACTION!definition}
\par}

In the third most common case,
`CheckTaskResult()' returns
`REDO_ACTION'.  When a
`REDO_ACTION' action is generated, the value of `taskInput' is
re-sent to the same slave that executed
`DoTask( taskInput )' for
the current task.  An application will typically invoke
`REDO_ACTION' if the environment has changed, and this changed
environment will produce a new `taskOutput'.  As before,
`DoTask()' then returns a new value of
`taskOutput'.  Then, `taskInput' and the new `taskOutput'
are again passed to
`CheckTaskResult()'.

Note that `MasterSlave()' guarantees that `REDO_ACTION'
causes the task to be re-sent to the same slave process.  This allows
the application to cache in a global variable some information
computed by the first invocation of `DoTask()'.  A second invocation of
`DoTask()' caused by the
`REDO_ACTION' allows the task to test if the `taskInput'
is the same as
the last invocation.  In that case, the application-defined `DoTask()'
routine can recognize that this is a `REDO_ACTION', and it can take
advantage of the cached global variable to avoid re-computing certain
quantities that do would not be changed by the altered environment.
In order to make this strategy possible, `MasterSlave()' also guarantees
that in the case of `REDO_ACTION', the slave process will not have
seen any intervening calls to `DoTask()' with values of
`taskInput' other than the current value.

In typical usage, the application-defined routine,
`CheckTaskResult()', will first call `IsUpToDate()'.
`IsUpToDate()' tests if the environment has been
modified since the current `taskInput' corresponding to
`CheckTaskResult()'
was originally generated by `SubmitTaskInput()'.  The times of the relevant
events are recorded as when seen on the master process.  It is an
error to call `IsUpToDate()' outside of a call to
`CheckTaskResult()' by `MasterSlave()'.
`IsUpToDate()' returns a boolean value, true or false.

The last possible action,
`CONTINUATION_ACTION( taskContinuation )', is provided for
unusual cases.  As with advice about the use of
``goto'', it is recommended to avoid `CONTINUATION_ACTION()'
where possible.  

A favorite aphorism of this author is, ``The source
code is the ultimate documentation''.  With this in mind, the reader
may also wish to read `lib/masslave.g', for which readability of the
code was one of the design criteria.

%======================================================================
\Section{Modifying Task Output or Input (a dirty trick)}

At this point, it should be noted that it explicitly *is* allowed
to modify the input or output of a task from within `CheckTaskResult()'.
This is not recommended in general, but there may be times when
`CheckTaskResult()' returns an `UPDATE_ACTION' and must also be used
to pass additional information to `UpdateSharedData()'.  In order to
modify a previous input or output, it is important that the
application has chosen a representation of the input or output as a
list or record, which can be modified in place, such that the
code excerpt succeeds without error.
\begintt
  oldOutput := taskOutput;
  # Modify taskOutput here
  if ( IsIdenticalObj( oldOutput, taskOutput ) ) = false then
    Error( "MasterSlave() will see only oldOutput, not current taskOutput" );
  fi;
  return UPDATE_ACTION;
\endtt

{\catcode 95=12
In principle, a *dirty trick* like this would also work in the case
of returning a `REDO_ACTION'.  However, this is not recommended.  For
that functionality, the code will be clearer if an explicit
`CONTINUATION_ACTION( modifiedTaskOutput )' is returned.  See
section~"CONTINUATION_ACTION() (the GOTO statement of the TOP-C model)"
for further discussion on the use of `CONTINUATION_ACTION()'.
\par}

%======================================================================
{\catcode 95=12
\Section{CONTINUATION_ACTION() (the GOTO statement of the TOP-C model)}
\par}

{\catcode 95=12
\>CONTINUATION_ACTION( taskContinuation )!{definition}
\par}

The `CONTINUATION_ACTION()', like the <goto> statement, is not
recommended for ordinary programs, but it may be useful in unusual
circumstances.  This is a parametrized action.  When the application
routine `GetTaskResult()' returns this action, `MasterSlave()'
guarantees to invoke `DoTask()' on the same slave process as for the
original task.  There will have been no intervening calls to
`DoTask()' on that slave, although there may have been an intervening
call to `UpdateSharedData()' on that slave.

This action allows arbitrary, repeated communication between the
master and a single slave process.  The slave process executes
`DoTask( taskInput )' and communicates with the master by returning a
`taskOutput'.  The master process executes `CheckTaskResult( taskInput,
taskOutput )' and returns a `taskContinuation'.  The original slave
process then receives another call to `DoTask( taskInput )', this time
with `taskInput' bond to `taskContinuation'.

%======================================================================
\Section{Being nice to other users (Nice, Alarm and LimitRss)}

When you are running a long job on a network of workstations, you will
often be sharing it with others.  Making your parallel job as unintrusive
as possible will leave you with a warmer welcome the next time that
you want to use that network of workstations.  Accordingly, three
useful functions are provided.

{\catcode 95=12
\>UNIX_Nice( <priority> )!{definition} F
\par}

This is similar to the `nice' command of many UNIX shells.  UNIX
priorities are in a range from -20 to 20 with -20 being the highest.
Users typically start at priority 0.  You can give yourself a lower
priority by specifying a priority of 5, for example.  Usually,
priorities 19 and 20 are *absolute* priorities.  Any process with
a priority higher than 19 that wishes to run will always have precedence.
Other priorities are *relative* priorities.  Your process will still
receive some CPU time even if other processes with higher priorities
are running.  You can set your priority lower, but you cannot raise
it back to its original value after that.  The return value is the
previous priority of your process.

{\catcode 95=12
\>UNIX_Alarm( <seconds> )!{definition} F
\par}

This causes the process to kill itself after that many seconds.
This is a useful safety measure, since it is unfortunately too easy
for a runaway slave process to continue if the master process is
killed without the normal `quit()'.  You might consider adding
something like `UNIX_Alarm( 25000 )' (about 6 hours) to your `.gaprc'
file.  Executing `UNIX_Alarm( 0 )' cancels any previous alarm.
The return value is the number of seconds remaining under the previous
setting of the alarm.

{\catcode 95=12
\>UNIX_LimitRss (<size> ) [ = setrlimit(RLIMIT_RSS, ...) ] F
\par}

Many dialects of UNIX (and their shells) offer a `limit' or `ulimit'
command to limit the resources available to the shell.  This command
limits the size of the RSS (resident set size), or the
amount of physical RAM used by your process.  The size limit is in
bytes.  Unfortunately, some UNIX dialects may not allow or even
silently ignore this request to limit the RSS.  A UNIX command
such as `top' can show you if your process RSS is staying below
your requested limit.

%======================================================================
\Section{Converting legacy sequential code to the TOP-C model}

The tutorial contains a section~"Raw MasterSlave (ParMultMat
revisited)", about <raw> version of MasterSlave() that is useful for
converting legacy sequential code to the TOP-C model.  However, that
model is not recommended for writing new code, for stylistic reasons.

%======================================================================
\Chapter{Tutorial using TOP-C and MasterSlave()}

\index{tutorial!TOP-C}
\index{tutorial!MasterSlave()}

This chapter assuems the background knowledge in section
"Basic TOP-C (Master-Slave) commands".
ParGAP/MPI must be invoked through the bin/pargap.sh script from the
distribution.  In addition, there must be
a procgroup file in the current directory when ParGAP is called.
Alternatively, the UNIX command line parameter: 
`-pr4pg FULL_PATH/procgroup' may be included on the pargap.sh command
line.  There is a sample procgroup file in `pkg/pargap/bin/' of the
distribution.  See the section "Invoking ParGAP/MPI with Remote Slaves"
for further details.  Many of the examples of this section can be
found in `pkg/pargap/examples/'.

%======================================================================
\Section{A simple example}

A master slave computation is invoked when a GAP program issues the
command MasterSlave().  This command is an example of what is called
``collective communication'' in MPI (although the command is not part of
MPI).  It is also sometimes called SPMD (Single Program, Multiple
Data), since all processes see the same code, although different
processes may execute different parts of the code.  The MasterSlave()
command must be invoked on all processes before execution can begin.
The following trivial example does this.  (Note that the final ``$\backslash$''
inside a string allows continuation of a string to the next line.)
We illustrate these principles first in their simplest form, making
all variables global variables.  Later, we introduce additional
ParGAP/MPI utilities that allow one to write in better style.

\beginexample
#Environment: none
#TaskInput:   counter
#TaskOutput:  counter^2 (square of counter)
#Task:        compute counter^2 from counter
result := [];
ParEval( "counter := 0; \
          SubmitTaskInput := function() \
            counter := counter + 1; \
            if counter <= 10 then return counter; else return NOTASK; fi; \
          end;");
ParEval( "DoTask := x->x^2" );
ParEval( "CheckTaskResultVers1 := function( input, output ) \
            result[input] := [input, output]; \
            return NO_ACTION; \
          end;" );
ParEval( "MasterSlave( SubmitTaskInput, DoTask, CheckTaskResultVers1 )" );
Print(result);
\endexample

By default, `MStrace = true', causing the execution to display each
input, `x', as it is passed from the master to a slave, and each output,
`x^2', as it is passed from the slave back to the master.  This behavior
can be turned off by setting: `MStrace := false;'
The fourth argument of
MasterSlave(), `Print', is a dummy argument that is never invoked
in this example.

Note that the result list is filled in only on the local process, and
was never defined or modified on the slave processes.  To remedy this
situation, we introduce the concept of an environment, a globally
shared, application-defined data structure.  A central principle of
the TOP-C model in ParGAP/MPI is that any routine may ``read'' the
environment, but the environment may be modified only by the
application-defined routine, `UpdateSharedData()'.
Hence, if we wanted the result list to be recorded on all processes
(perhaps as a lookup table), we would now write:

%======================================================================
\Section{ParSquare}

Until now, we have been using global variables.  It is better style to
use local variables, where possible.  We rewrite the above routine in
the improved style:

\beginexample
#Environment: result [ result is shared among all processes ]
#TaskInput:   counter
#TaskOutput:  counter^2 (square of counter)
#Task:        compute counter^2 from counter
#UpdateSharedData:  record [counter, counter^2] at index, counter, of result
MSSquare := function( range ) # expects as input a range, like [1..10]
  local counter, result,
      SubmitTaskInput, DoTask, CheckTaskResultVers2, UpdateSharedData;
  counter := range[1]; # Reset counter for use in SubmitTaskInput()
  result := [];
  SubmitTaskInput := function()
    counter := counter + 1;
    if counter <= range[Length(range)] then return counter;
    else return NOTASK;
    fi;
  end;
  DoTask := x->x^2;
  CheckTaskResultVers2 := function( input, output )
    return UPDATE_ACTION;
  end;
  UpdateSharedData := function( input, output )
    result[input] := [input, output];
  end;
  MasterSlave( SubmitTaskInput, DoTask, CheckTaskResultVers2, UpdateSharedData );
  return result;
end;

#ParSquare() is the main calling function;  It must define MSSquare on
#  all slaves before calling it in parallel.
ParSquare := function( range ) # expects as input a range, like [1..10]
  ParEval( PrintToString( "MSSquare := ", MSSquare ) );
  return ParEval( PrintToString( "MSSquare( ", range, ")" ) );
end;
\endexample

%======================================================================
\Section{ParInstallTOPCGlobalFunction() and TaskInputIterator()
         (ParSquare revisited)}

This example can be written more compactly by using some of the
convenience functions provided by ParGAP/MPI.  Specifically, we would
rewrite this as:

\index{ParInstallTOPCGlobalFunction!example}
\index{example!ParInstallTOPCGlobalFunction}
\index{TaskInputIterator!example}\index{example!TaskInputIterator}

\beginexample
ParInstallTOPCGlobalFunction( "ParSquare", function( range )
  local result;
  result := [];
  MasterSlave( TaskInputIterator( range ),
               x->x^2,
               function( input, output ) return UPDATE_ACTION; end, 
               function( input, output ) result[input] := [input, output]; end
             );
  return result;
end );
\endexample

The usage above demonstrates the use of two utilities.

% \>ParInstallTOPCGlobalFunction( <gvar>, <function> ) F
\>ParInstallTOPCGlobalFunction( <string>, <function> )!{definition} F

This defines <gvar> as a function on the master and on the slaves.
On each slave, the definition of <gvar> is given by <function>.
However, on the master, <gvar> is defined as a function that first
calls <gvar> on all slaves with the arguments originally passed
to <gvar>, and then on the master, <function> is called with the
original arguments.  This is exactly the behavior that is wanted
in order to compress an invocation of `MasterSlave()' so that the
right things happen on both the master and on the slaves.  This
is exactly what we saw in the previous definition of `ParSqure',
above.

\>TaskInputIterator( <collection> ) F

This function provides the functionality of a common case of
`SubmitTaskInput()', by turning it into a GAP iterator
(see~"ref:iterators").  Its meaning is best understood from its
definition:

\begintt
TaskInputIterator := function( collection )
  local iter;
  iter := Iterator( collection );
  return function()
           if IsDoneIterator(iter) then return NOTASK;
           else return NextIterator(iter);
           fi;
         end;
end;
\endtt

%======================================================================
\Section{ParMultMat}

Let us now write a matrix-matrix
multiplication routine in this style.  Since matrix multiplication for
dimension $n$ requires $n^3$ operations, we can afford to spend $n^2$ time
doing any sequential work.  (A finer analysis would also consider the
number of slaves, $k$, resulting in up to $k*n^2$ time to send all
messages, depending on the MPI broadcast algorithm.)  So, a sequential
matrix multiplication program might be written as follows.  (The style
emphasizes clarity over efficiency.)

\index{example!SeqMultMat}

\beginexample
SeqMultMat := function(m1, m2)               # sequential code
  local i, j, k, n, m2t, sum, result;
  n := Length(m1);
  result := [];
  m2t := TransposedMat(m2);
  for i in [1..n] do
    result[i] := [];
    for j in [1..n] do
      sum := 0;
      for k in [1..n] do
        sum := sum + m1[i][k]*m2t[j][k];
      od;
      result[i][j] := sum;
    od;
  od;
  return result;
end;
\endexample

We choose to define the task as the computation of a single row of the
result matrix.  This corresponds to the body of the outermost loop.

\index{example!ParMultMat}

\beginexample
#Environment: m1, m2t, result (three matrices)
#TaskInput:   i (row index of result matrix)
#TaskOutput:  result[i] (row of result matrix)
#Task:        Compute result[i] from i, m1, and m2
#UpdateSharedData:  Given result[i] and i, modify result on all processes.

ParInstallTOPCGlobalFunction( "ParMultMat", function(m1, m2)
  local i, n, m2t, result, DoTask, CheckTaskResult, UpdateSharedData;
  n := Length(m1);
  result := [];
  m2t := TransposedMat(m2);

  DoTask := function(i) # i is task input
    local j, k, sum;
    result[i] := [];
    for j in [1..n] do
      sum := 0;
      for k in [1..n] do
        sum := sum + m1[i][k]*m2t[j][k];
      od;
      result[i][j] := sum;
    od;
    return result[i]; # return task output, row_i
  end;
  # CheckTaskResult executes only on the master
  CheckTaskResult := function(i, row_i) # task output is row_i
    return UPDATE_ACTION; # Pass on output and input to UpdateSharedData
  end;
  # UpdateSharedData executes on the master and on all slaves
  UpdateSharedData := function(i, row_i) # task output is row_i
    result[i] := row_i;
  end;
  # We're done defining the task.  Let's do it now.
  MasterSlave( TaskInputIterator([1..n]), DoTask, CheckTaskResult,
               UpdateSharedData );
  # result is defined on all processes;  return local copy of result
  return result;
end );
\endexample

%======================================================================
\Section{DefaultCheckTaskResult (as illustrated by ParSemiEchelonMatrix)}

Now that the basic principles of the TOP-C model are clear, we
investigate an example that requires most of the basic features of
ParGAP/MPI, including the use of `IsUpToDate()' and
`REDO_ACTION'.  Recall the standard idiom for
`CheckTaskResult()'.  These issues were discussed in
the section~"Efficient Parallelism in MasterSlave() using CheckTaskResult()".

\begintt
DefaultCheckTaskResult := function( taskOutput, taskInput )
  if taskOutput = false then return NO_ACTION;
  elif not IsUpToDate() then return REDO_ACTION;
  else return UPDATE_ACTION;
  fi;
end;
\endtt

The next example is a parallelization of the function
`SemiEchelonMat()' (a form of Gaussian elimination) in the GAP
library, `lib/matrix.gi'.  Unlike the previous examples, parallelizing
Gaussian elimination efficiently is a non-trivial undertaking.  This
is because a naive parallelization has poor load balancing.  A slave
executing a task in the middle will have to <sift> a row vector
through many previous row vectors, while a slave executing a task
toward the beginning or end will have little work to do.  We will
begin with a naive parallelization based on the sequential code, and
then migrate the code in a natural manner toward a more efficient
form, by analyzing the inefficiencies and applying the TOP-C model.

The reader may wish to stop and read the original code in
`lib/matrix.gi' first.  The logic of `SemiEchelonMat()' is to examine
each row vector of an input matrix, in order, reduce it by a list of
basis vectors stored in `vectors', and then add the row to `vectors'.
Upon completion, the number of leading zeroes of the row vectors in
`vectors' may not increase monotonically, but each element of
`vectors' will have a unique number of leading zeroes.  Some rows of
the input matrix may reduce to the zero matrix, in which case they are
not added to `vectors'.

For the reader's convenience, the original sequential code is
reproduced here.

\beginexample
SemiEchelonMat := function( mat )

    local zero,      # zero of the field of <mat>
          nrows,     # number of rows in <mat>
          ncols,     # number of columns in <mat>
          vectors,   # list of basis vectors
          heads,     # list of pivot positions in 'vectors'
          i,         # loop over rows
          j,         # loop over columns
          x,         # a current element
          nzheads,   # list of non-zero heads
          row;       # the row of current interest

    mat:= List( mat, ShallowCopy );
    nrows:= Length( mat );
    ncols:= Length( mat[1] );

    zero:= Zero( mat[1][1] );

    heads:= ListWithIdenticalEntries( ncols, 0 );
    nzheads := [];
    vectors := [];

    for i in [ 1 .. nrows ] do
        row := mat[i];
        # Reduce the row with the known basis vectors.
        for j in [ 1 .. Length(nzheads) ] do
            x := row[nzheads[j]];
            if x <> zero then
                AddRowVector( row, vectors[ j ], - x );
            fi;
        od;
        j := PositionNot( row, zero );
        if j <= ncols then
            # We found a new basis vector.
            MultRowVector(row, Inverse(row[j]));
            Add( vectors, row );
            Add( nzheads, j);
            heads[j]:= Length( vectors );
        fi;
    od;

    return rec( heads   := heads,
                vectors := vectors );
    end;
\endexample

\index{example!ParSemiEchelonMat}
\index{example!parallel Gaussian elimination}
\index{Gaussian elimination!parallel}

Although GAP{\pif}s Gaussian elimination algorithm appears to be
awkward to parallelize (since the next row vector in `vectors' depends
on row reduction by all previous vectors, we will see how the original
code of `SemiEchelonMat()' can be modified in a natural manner to
produce useful parallelism.  This illustrates the general TOP-C
paradigm for <naturally> parallelizing a sequential algorithm.

\beginexample
#Environment: vectors (basis vectors), heads, nzheads, mat (matrix)
#TaskInput:   i (row index of matrix)
#TaskOutput:  List of (1) j and (2) row i of matrix, mat, reduced by vectors
#               j is the first non-zero element of row i
#Task:        Compute reduced row i from mat, vectors, heads
#UpdateSharedData:  Given i, j, reduced row i, add new basis vector
#               to vectors and update heads[j] to point to it

ParInstallTOPCGlobalFunction( "ParSemiEchelonMat", function( mat )
    local zero,      # zero of the field of <mat>
          nrows,     # number of rows in <mat>
          ncols,     # number of columns in <mat>
          vectors,   # list of basis vectors
          heads,     # list of pivot positions in 'vectors'
          i,         # loop over rows
          nzheads,   # list of non-zero heads
          DoTask, UpdateSharedData;

    mat:= List( mat, ShallowCopy );
    nrows:= Length( mat );
    ncols:= Length( mat[1] );

    zero:= Zero( mat[1][1] );

    heads:= ListWithIdenticalEntries( ncols, 0 );
    nzheads := [];
    vectors := [];

    DoTask := function( i ) # taskInput = i
      local j,         # loop over columns
            x,         # a current element
            row;       # the row of current interest
      row := mat[i];
      # Reduce the row with the known basis vectors.
      for j in [ 1 .. Length(nzheads) ] do
          x := row[nzheads[j]];
          if x <> zero then
              AddRowVector( row, vectors[ j ], - x );
          fi;
      od;
      j := PositionNot( row, zero );
      if j <= ncols then return [j, row]; # return taskOutput
      else return fail; fi;
    end;
    UpdateSharedData := function( i, taskOutput )
      local j, row;
      j := taskOutput[1];
      row := taskOutput[2];
      # We found a new basis vector.
      MultRowVector(row, Inverse(row[j]));
      Add( vectors, row );
      Add( nzheads, j);
      heads[j]:= Length( vectors );
    end;
    
    MasterSlave( TaskInputIterator( [1..nrows] ), DoTask, DefaultCheckTaskResult,
                  UpdateSharedData );

    return rec( heads   := heads,
                vectors := vectors );
end );
\endexample

The next section describes how to make this code more efficient.

%======================================================================
\Section{Caching slave task outputs (ParSemiEchelonMat revisited)}

The code above is inefficient unless `nrows >> ncols'.  This is
because if `nrows' is comparable to `ncols', it will be rare for
`DoTask()' to return `fail'.  If most slaves return a result distinct
from `fail', then `DefaultCheckTaskResult()' will return an
`UPDATE_ACTION' upon receiving the output from the first slave, and it
will return a `REDO_ACTION' to all other slaves, until those slaves
execute `UpdateSharedData()'.  The inefficiency arose becaseu a
`REDO_ACTION' caused the original slave process to re-compute
`DoTask()' from the beginning.

\index{example!ParSemiEchelonMat revisited}

In the case of a `REDO_ACTION', we can fix this by taking advantage of
information that was already computed.  To accomplish this, a global
variable should be defined on all slaves:
\beginexample
ParEval("globalTaskOutput := [[-1]]");
\endexample
the routine `DoTask()' in the previous example should be modified to:

\beginexample
  DoTask := function( i )
      local j,         # loop over columns
            x,         # a current element
            row;       # the row of current interest
    if i = globalTaskOutput[1] then
      # then this is a REDO_ACTION
      row := globalTaskOutput[2]; # recover last row value
    else row := mat[i];
    fi;
    # Reduce the row with the known basis vectors.
    for j in [ 1 .. Length(nzheads) ] do
      x := row[nzheads[j]];
      if x <> zero then
        AddRowVector( row, vectors[ j ], - x );
      fi;
    od;
    j := PositionNot( row, zero );
    # save row in case of new REDO_ACTION
    globalTaskOutupt[1] := i;
    globalTaskOutput[2] := row;
    if j <= ncols then return [j, row]; # return taskOutput
    else return fail; fi;
  end;
\endexample

(A perceptive reader will have noticed that it was not necessary to
also save and restore `row' from `globalTaskOutput', since this can
be found again based on the saved variable value `i'.  However, the
additional cost is small, and it illustrates potentially greater
generality of the method.)

The next section describes how to make this code more efficient.

%======================================================================
\Section{Agglomerating tasks for efficiency
         (ParSemiEchelonMat revisited again)}

\index{taskAgglom}\index{agglomerating tasks}\index{TaskAgglomIndex!context}
\index{example!taskAgglom}

A more efficient parallelization would partition the matrix into sets
of adjacent rows, and send an entire set as a single `taskInput'.
This would minimize the communication overhead, since the network latency
varies only slowly with message sizxe, but linearly with the number
of messages.  To minimize network latency, one adds an extra parameter
to `MasterSlave()' in order to bundle, perhaps, up to 5 tasks at a time.

\beginexample
    MasterSlave( TaskInputIterator( [1..n] ), DoTask, DefaultCheckTaskResult,
                  UpdateSharedData, 5 );
\endexample

Now the task input will be a list of the next 5 tasks returned by
`GetTaskInput()', or in this case by `TaskInputIterator( [1..nrows] )'.
If fewer than 5 tasks are produced before `NOTASK' is returned, then
the task input will be correspondingly shorter.  If the first input
task is `NOTASK' (yielding a list of tasks of length 0), then this
will be interpreted as a traditional `NOTASK'.  The task output
corresponding to this task input is whatever the application routine,
`DoTask()' produces as task output.  The routine `DoTask()' will be
unchanged, and `MasterSlave()' will arrange to repeatedly call
`DoTask()', once for each input task and produce a list of task
outputs.

Hence, this new variation requires us to rewrite
`UpdateSharedData()' in the obvious manner, to handle a list of input
and output tasks.  Here is one solution to patch the earlier code.

\>TaskAgglomIndex V

This global variable is provided for use inside `DoTask()'.  It allows
the application code to inquire about the index of the input task in
the full list of tasks created when agglomTask is used.  The variable
is most useful in the case of a `REDO_ACTION' or
`CONTINUATION_ACTION()', as illustrated below.

\index{example!ParSemiEchelonMat revisited again}

\beginexample
ParEval("globalTaskOutputs := []");

#Environment: vectors (basis vectors), heads, mat (matrix)
#TaskInput:   i (row index of matrix)
#TaskOutput:  List of (1) j and (2) row i of matrix, mat, reduced by vectors
#               j is the first non-zero element of row i
#Task:        Compute reduced row i from mat, vectors, heads
#UpdateSharedData:  Given i, j, reduced row i, add new basis vector
#               to vectors and update heads[j] to point to it

ParInstallTOPCGlobalFunction( "ParSemiEchelonMat", function( mat )
  local zero,      # zero of the field of <mat>
        nrows,     # number of rows in <mat>
        ncols,     # number of columns in <mat>
        vectors,   # list of basis vectors
        heads,     # list of pivot positions in 'vectors'
        i,         # loop over rows
        nzheads,   # list of non-zero heads
        DoTask, UpdateSharedDataWithAgglom;

  mat:= List( mat, ShallowCopy );
  nrows:= Length( mat );
  ncols:= Length( mat[1] );

  zero:= Zero( mat[1][1] );

  heads:= ListWithIdenticalEntries( ncols, 0 );
  nzheads := [];
  vectors := [];

  DoTask := function( i )
      local j,         # loop over columns
            x,         # a current element
            row;       # the row of current interest
    if IsBound(globalTaskOutputs[TaskAgglomIndex])
        and i = globalTaskOutputs[TaskAgglomIndex][1] then
      # then this is a REDO_ACTION
      row := globalTaskOutputs[TaskAgglomIndex][2][2]; # recover last row value
    else row := mat[i];
    fi;
    # Reduce the row with the known basis vectors.
    for j in [ 1 .. Length(nzheads) ] do
      x := row[nzheads[j]];
      if x <> zero then
        AddRowVector( row, vectors[ j ], - x );
      fi;
    od;
    j := PositionNot( row, zero );

    # save [input, output] in case of new REDO_ACTION
    globalTaskOutputs[TaskAgglomIndex] := [ i, [j, row] ];

    if j <= ncols then return [j, row]; # return taskOutput
    else return fail; fi;
  end;
  
  # This version of UpdateSharedData() expects a list of taskOutput's
  UpdateSharedDataWithAgglom := function( listI, taskOutputs )
    local j, row, idx, tmp;
    for idx in [1..Length( taskOutputs )] do
      j := taskOutputs[idx][1];
      row := taskOutputs[idx][2];
      
      if idx > 1 then
        globalTaskOutputs[1] := [-1, [j, row] ];
        tmp := DoTask( -1 ); # Trick DoTask() into a REDO_ACTION
        if tmp <> fail then
          j := tmp[1];
          row := tmp[2];
        fi;
      fi;

      # We found a new basis vector.
      MultRowVector(row, Inverse(row[j]));
      Add( vectors, row );
      Add( nzheads, j);
      heads[j]:= Length( vectors );
    od;
  end;
    
  MasterSlave( TaskInputIterator( [1..nrows] ), DoTask, DefaultCheckTaskResult,
                UpdateSharedDataWithAgglom, 5 ); #taskAgglom set to 5 tasks

  return rec( heads   := heads,
              vectors := vectors );
end );
\endexample

Note that in this simple example, we were able to re-use most of the
code from the previous version, at the cost of adding an additional
global variable, `globalTaskOutputs'.  In fact, the last `DoTask()' is
backward compatible to the first version of the code, for which
`agglomTasks' is not used.  If we wanted to run the latest code
without agglomeration of tasks, it would suffice either to set
the `taskAgglom' parameter to 1, or else to remove it entirely and
replace `UpdateSharedDataWithAgglom()' by `UpdateSharedData()'.

It is useful to experiment with the above code by substituting a
variable, `taskAgglom', for the number 5, and trying it out with
remote slaves on your own network for different values of `taskAgglom'
and for different size matrices.  You can call `MasterSlaveStats()'
to see the effect of different parameters.  Suitable pseudo-random matrices
can be quickly generated via `mat := PseudoRandom( GL( 30, 5 )' and similar
commands.


The paper~\cite{Coo98} is suggested as further reading to see
a still more efficient parallel implementation
of ParSemiEchelonMatrix (also known as Gaussian elimination)
using the TOP-C model.

%======================================================================
\Section{Raw MasterSlave (ParMultMat revisited)} 

\index{raw MasterSlave()!definition}
\index{MasterSlave()!raw}
\index{example!ParMultMat revisited}

Finally, we given an example of a variation of `MasterSlave()', based
on a <raw> `MasterSlave()'.  These versions are designed for the
common case of legacy code that contains deeply nested parentheses.
The `taskInput' may be generated inside several nested loops, making
it awkward and error-prone to produce a function, `SubmitTaskInput()',
that will generate instances of `taskInput' in the appropriate
sequence.

Effectively, when we wish to return successive values from several deeply
nested loops, we are in the situation of programming the opposite of a
gap <iterator> (see~"ref:iterators").  We are already producing
successive iterator values, and we wish to <stuff them back into some
iterator>.  Until GAP develops such a language construct :-) , the
following example of a <raw> `MasterSlave()' demonstrates a solution.
Before studying this example, please review the sequential version,
`SeqMultMat()' near the beginning of section~"ParMultMat".

\index{raw MasterSlave()!example}\index{example!raw MasterSlave()}

We make use of the following three new ParGAP/MPI functions.

\>BeginRawMasterSlave( DoTask, CheckTaskResult, UpdateSharedData ) F
\>RawSubmitTaskInput( taskInput ) F
\>EndRawMasterSlave() F

Their use will be obvious in the next example.  This time, we
parallelize `SeqMultMat()' by defining the task as the computation of a
single entry in the result matrix.  Hence, the task will be the
computation of the appropriate inner product.  For dimension $n$,
$n^2$ tasks are now generated, and each task is generated inside a
doubly nested loop.

\beginexample
#Environment: m1, m2t, result (three matrices)
#TaskInput:   [i,j] (indices of entry in result matrix)
#TaskOutput:  result[i][j] (value of entry in result matrix)
#Task:        Compute inner produce of row i of m1 by colum j of m1
              ( Note that column j of m1 is also row j of m2t, the transpose )
#UpdateSharedData:  Given result[i][j] and [i,j], modify result everywhere

ParInstallTOPCGlobalFunction( "ParRawMultMat", function(m1, m2)
  local i, j, k, n, m2t, sum, result, DoTask, CheckTaskResult, UpdateSharedData;
  n := Length(m1);
  m2t := TransposedMat(m2);
  result := ListWithIdenticalEntries( Length(m2t), [] );

  DoTask := function( input )
    local i,j,k,sum;
    i:=input[1]; j:=input[2];
    sum := 0;
    for k in [1..n] do
      sum := sum + m1[i][k]*m2t[j][k];
    od;
    return sum;
  end;

  CheckTaskResult := function( input, output )
    return UPDATE_ACTION;
  end;

  UpdateSharedData := function( input, output )
    local i, j;
    i := input[1]; j := input[2];
    result[i][j] := output;
    # result[i,j] := sum;
  end;

  BeginRawMasterSlave( DoTask, CheckTaskResult, UpdateSharedData );
  for i in [1..n] do
    result[i] := [];
    for j in [1..n] do
      RawSubmitTaskInput( [i,j] );
      # sum := 0;
      # for k in [1..n] do
      #   sum := sum + m1[i][k]*m2t[j][k];
      # od;
      # result[i][j] := sum;
    od;
  od;
  EndRawMasterSlave();

  return result;
end );
\endexample

% Should also add section with continuation example.
% Should also add section with search example, and example with
%    coset enumeration (coset enum requires loading additional modified
%    C to check coset table and report work to do without changing things
%    since the coset table will be the environment.
% Possibilities for search example include naive breadth-first search
%    parallelization, which is fine if have a long wavefront and each slave
%    takes a piece of the wavefront, or else CILK-style work-stealing
%    paradigm, maybe even with CILK 8-queens example.

%======================================================================
\Chapter{Advanced Concepts for TOP-C (Master-Slave)}


This chapter may be safely skipped on a first reading.  If you still
want to read this chapter, it should mean that you are familiar with
the basics of the TOP-C model, and are looking for advice on how to
use the model more effectively.  The first piece of advice is that the
choice of task and environment interact strongly with the choice of
parallel algorithm.  We review those concepts more precisely here, in
light of the overall context of the TOP-C model.

\index{task}

<TASK>:  A task is a function that that takes a single argument, <task
input>, reads certain globally shared data, the <environment>, and
computes a result, the <task output>.  Hence, given the same task input and
the same environment, a task should always compute the same task
output.  The TOP-C model implements this concept through the
`DoTask()' application routine.
In the TOP-C model, this rule is bent to accomodate caching of private
data to efficiently handle a <REDO_ACTION> (see~"Caching slave task
outputs (ParSemiEchelonMat revisited)"), or to accomodate a
<CONTINUATION_ACTION()>
{\catcode 95=12
(see~"CONTINUATION_ACTION() (the GOTO statement
of the TOP-C model)").
\par}

\index{environment}

<ENVIRONMENT>: An environment is globally shared data.  It should be
initialized before entering `MasterSlave()'.  The environment is never
explicitly declared.  However, it is best for the application
programmer to include a comment specifying the environment for his or
her application.  The TOP-C model poses certain restrictions on a
legal environment.  The environment must include enough of the global
data (variables that occur <free> in the `DoTask()' procedure) so that
the task output of `DoTask()' is uniquely determined by the task input
and the environment.  However, the environment must *not* include any
variables whose values are modified outside of the application routine
`UpdateSharedData()'.  Also, the environment is updated
*non-preemptively*, in the sense that a slave process will always
complete its current task before reading a newly arrived message that
invokes `UpdateSharedData()'.  If a slave privately caches data for
purposes of a <REDO_ACTION> or <CONTINUATION_ACTION()>, such data is
explicitly not part of the environment.

%======================================================================
\Section{Tracing and Debugging}\label{debug}

In testing a program using `MasterSlave()', a hierarchy of testing is
suggested.  The principle is to test the simplest example first, and
then iterate to more complex examples.  When a stable portion of the
program is ready for testing, the following sequence of tests is
suggested:

\beginitems
sequential& Replace `MasterSlave()' by `SeqMasterSlave()' (see
definition below) and see if the program performs correctly.
`SeqMasterSlave()' will run only on the master, without sending any
messages, and so the full range of sequential debugging tools is
available.

one slave& Restore `MasterSlave()' and set up the `procgroup' file to
have only one slave process (one line, `local 0', and one line
`localhost ...').  Initially test with no `taskAgglom' parameter for
`MasterSlave()', and then test with the full set of parameters.

two slaves& Same advice as for one slave, but two lines: `localhost ...'

many slaves& Full scale test, both without and with `taskAgglom'.

\enditems

\>ParTrace V

A second easy testing strategy is to set `ParTrace' to true.  (This is
the default value.)  This causes all <task inputs>, <task outputs>,
and non-trivial <actions> (actions other than `NO_ACTION') to be
displayed at the terminal.  The information is printed in the same
sequence as seen by the master process.

Another ``cheap'' debugging trick is to inspect the values
of global variables on the slave after it has been thrown out of the
`MasterSlave()' procedure. The following code demonstrates by
interrogating the sum of the variables `x' and `y' on slave number~2.
\begintt
    SendRecvMsg("x+y;\n", 2);
\endtt
This is useful to inspect cached data on a slave used for a
<REDO_ACTION> or <CONTINUATION_ACTION()>.  It may also be useful to
verify if the environment on the slave is the same as on the master.
If the slave process is still inside the procedure `MasterSlave()', then
from within a break loop on the master, you may also want to
interactively call `DoTask( <test input> )' to determine if the
expected <task output> is produced.

If the master process is still within `MasterSlave()', then it is
useful to execute `DoTask()' locally on the master process,
and debug this sequentially.

There is also the time-honored practice of inserting print statements.
Print statements ``work'' both on the master and on the slaves.
If ParTrace produces too much output, or not the right kind of information, one
can add print statements exactly where one needs them.
As with any UNIX debugging, it is sometimes useful to
include a call to `fflush(stdout)' to force any pending output.
ParGAP/MPI binds this to:

{\catcode 95=12
\>UNIX_FflushStdout()!{definition} F
\par}

This has the same effect as the UNIX `fflush(stdout)'.  There may be
pending output in a buffer, that UNIX delays printing for efficiency.
Printing any remaining output in the buffer is forced by this command.
A common sequence is: `Print("information"); UNIX_FflushStdout();'.
Note also that when the slave prints, there are ``two'' standard outputs
involved.  You may also want to include a call to `UNIX_FflushStdout()'
on the master to force any pending output that originated on a slave.
Finally, you should be conscious of network delays, and so a print
statement in a slave process will typically take longer to appear than a print
statement in the master process.

\>SeqMasterSlave( <SubmitTaskInput>, <DoTask>[, <CheckTaskResult>[,
                  <UpdateSharedData>[, <taskAgglom> ]]] )!{definition} F

If a bug is exhibited even in the context of a single slave, then
the code is ``almost'' sequential.  In this case,
one can test further by replacing the call to `MasterSlave()' by
a call to `SeqMasterSlave()', and debug in a context that involves
zero messages and no interaction with any slave.  It can also be helpful
to carry out initial debugging in this context.  Note that in the
case of a single slave, which is what `SeqMasterSlave' emmulates,
`IsUpToDate()' will always return true, and so most applications will
not call for a <REDO_ACTION>.

%======================================================================
\Section{Efficiency Considerations}\label{efficiency}

There are two common reasons for loss of efficiency in parallel
applications.  One is a lack of enough tasks, so tthat some slaves are
starverd for work while waiting for the next task input.  A second
reson is that the ration of communication time to compilation time is
too large.  The second case, poor communication efficiency, is the
more common one.

The communication efficiency can be formally defined as the
ratio of the time to execute a task by the time taken for the master to
send an initial task message to a slave plus the time for the slave to send
back a result message.  A good way to diagnose your efficiency is
to execute `MasterSlaveStats()' after executing `MasterSlave()'.

\>MasterSlaveStats() F

This function currently returns statistics in the form of a list
of two records.  The first record provides the global information:
\beginitems
+& `MStime' total runtime (as measured by `Runtime()')

+& `MSnrTasks' total number of tasks (not including `REDO' or `UPDATE'

+& `MSnrUpdates' total number of times action `UPDATE' was returned

+& `MSnrRedos' total number of times action `REDO' was returned
\enditems
The second record provides per-slave information:
\beginitems
+& `total' (total time spent on tasks, not including `UpdateSharedData()')

+& `num' (number of initial tasks, `REDO' and `CONTINUATION()' actions)

+& `ave_ms' (`QuoInt(1000\*total,num)' )

+& `max' (maximum time spent on a task, in seconds)
\enditems

Note that for purposes of the per-slave statistics, separate time intervals
are recorded for each initial task, `REDO' action, and `CONTINUATION()'
action.  The time for `UpdateSharedData()' is not included in these
statistics.  This is because after an `UPDATE' action, the slave does
not reply to the master to acknowledge when the update was completed.

{\it

Poor communication efficiency is typically caused either by too small a task
execution time (which would be the case in the example of
section or too large a message (in which case the
communication time is too long).  We first consider execution times that are
too small.

On many Ethernet installations, the communication time is about 0.01
seconds to send and receive small messages (less than 1 Kb).  Hence the
task should be adjusted to consume at least this much CPU time.  If the
naturally defined task requires less than 0.01 seconds, the user can often
group together several consecutive tasks, and send them as a single larger
task.  For example, in the factorization problem of section, one
might modify `DoTask()' to test the next 1000 numbers as factors and
modify `SubmitTaskInput()' to increment `counter' by~1000.

There is another easy trick that often improves communication efficiency.
This is to set up more than one slave process on each processor.  This
improves the communication efficiency because during much of the typical
0.01~seconds of communication time the CPU has off-loaded the job onto a
coprocessor.  Hence, having a second slave process running its own task on the
CPU while a first process is concerned with communication allows one to
{\it overlap communication with computation}.

We next consider the case of messages that are too large.  In this case, it
is important to structure the problem appropriately.  The task architecture
is intended to be especially adaptable to this case.  The philosophy is to
minimize communication time by duplicating much of the execution time on
each processor.

Hence, rather than build a large initial global data structure
on the master and then send it to the slaves, it is often better to build the
initial data structure on each processor independently, since this requires
no communication.  This is why `InitDataStructure()' from
section was executed on the master and all slaves.  This requires
less communication overhead than executing `InitDataStruct()' once on the
master and sending the result to all slaves.

After the initial data structure has been built, it will usually be
modified as a result of the computation.  In order to again minimize
communication, the result of a task, which is typically passed to
`UpdateSharedData()', should consist of the minimum information
needed to update the global data structure.  Each process can then
perform this update in parallel.

}

%======================================================================
\Section{Checkpointing in TOP-C}

Any long-runnning computation *must* be concerned with checkpointing.
The TOP-C model also provides a simple model for checkpointing.
The key observation is that the master process always has the latest
state of the computation, and the information in the master process
is sufficient to reconstruct any ongoing computation.  Any application
may take advantage of this by checkpointing the necessary information
either in the application routine, `SubmitTaskInput()' or `CheckTaskResult()'.

A simple way to checkpoint is to record:
\beginitems
+& the current data in the TOP-C environment;
+& any private global data residing only in the master process; and
+& the inputs to any tasks that are still pending.
\enditems
This model for checkpointing assumes that your program has no
`CONTINUATION()' actions.  If you use `CONTINUATION()' actions, then
you may require a more complex model for checkpointing.

\>MasterSlavePendingTaskInputs() F

This function returns a GAP list (with holes) of all pending task inputs.
If slave `i' is currently working on a task, index `i' of the list
will record that task.  If slave `i' is currently idle or executing
`UpdateSharedData()', then there will be a hole at index `i'.
This function is available for use within either the application
routine `SubmitTaskInput()', or `CheckTaskResult()', as specified in the
parameters of your call to `MasterSlave()'.  (Of course, your application
may be using a name other than `SubmitTaskInput()' or `CheckTaskResult()'
in the parameters of `MasterSlave()'.)

%======================================================================
\Section{When Should a Slave Process be Considered Dead?}

An important question for long-running computations, is when to
decide that a slave process is dead.  For our purposes, *dead* is
not a well-defined concept.  If a user on the remote machine decides
to re-boot, it is clear that any slave processes residing on that
machine should be declared dead.  However, suppose there is temporary
congestion on the network making the slave unavailable.  Suppose
that another user on the remote machine has started up many processes
consuming many resources, and the TOP-C slave process is being starved
for CPU time or for RAM.  Perhaps the most difficult case of all to
decide is if one particular TOP-C task requires ten times as much
time as all other tasks.  This last example is conceivable if, for example,
each task consists of factoring a different large integer.

Hence, our implementation of the TOP-C model will employ the following
*heuristic* in a future version, to decide if a task is dead.  You may
wish to employ this heuristic now, if you have a demanding application.
We use the ParGAP function, `UNIX_Realtime()', to keep track of how
much time has been spent on a task (based on ``wall clock time'', and not
on CPU time).  If a task has taken `slaveTaskTimeFactor' times as
much time as the longest task so far, then it becomes a candidate
for being declared dead.  The GAP variable `slaveTaskTimeFactor' is
initially set to the default value of 2.

Once a slave process becomes a candidate for being declared dead,
`MasterSlave()' will create a second version of the same task, with the
same task input as the original task.  `MasterSlave()' will then record
which task finishes first.  If the original version finishes first, then
the second version of the task is ignored, and the slave process executing
the original task is no longer considered a candidate for death.

If, however, the second version of the task finishes before the
original version, then the time for the second task is recorded.
Further, the output from the second task will be used, and any output
resulting from the original task will be ignored.  `MasterSlave()' then
periodically checks until the ration of the time spent so far on the
original version of the task is at least `slaveTaskTimeFactor' times
greater than the time spent on the second version of the task, then the
process executing the original version of the task is then declared
dead.  No further messages from the process executing original task
will be recognized and no further messages will be sent to that slave
process.

A future version of this distribution will include direct support for
this heuristic.  A customized version of it may be used now, by taking
advantage of the ParGAP/MPI routine, `UNIX_Realtime()'.  In addition,
a future version of this distribution may include the ability to
start new slave processes in an ongoing computation.  The
reference~\cite{CG98} describes how this was done in a C implementation,
and why this concept fits naturally with the TOP-C model.

% %======================================================================
% \Section{Advanced Features}
% 
% {\it
% 
% Sometimes MasterSlave is asymmetric in that the master may have more
% RAM and be able to hold more data.
% 
% Next, there are cases when the ``natural'' algorithm requires frequent
% recursive calls.  This is the case with construction of strong generating sets
% for permutation groups and with construction of an AG generating
% sequence for AG groups (polycyclically presented groups).
% With care, one can also include
% a recursive call to `MasterSlave()' inside `CheckTaskResult()'
% {\bf or maybe UpdateSharedData()}.  But there are issues, such as
% being reentrant (save slaveArray and restore, worry about preivous
% pending tasks and whether to kill them and start over, or save them).
% It's also related to search, for which CILK work-stealing is also an
% option.
% 
% If communication time is too much, then you should break up any very large
% messages into elementary MPI datatypes.  So, consider breaking up messages of
% over several thousand bytes into several messages, where each large messages
% is of LISP type: (vector fixnum) or (vector float).
% 
% large space is a problem (e.g: large lookup table, cite condensation
% paper);
% 
% save lookups, use CONTINUATION_ACTION(), IsMaster() for asymmetric
% stuff inside UpdateSharedData
% 
% =========================
% 
% For really alrge space, generalized model under development
% 
% =========================
% 
% Also dynamic processes, better tolerance of dying slaves in works for
% next version.
% 
% }

%======================================================================
\Chapter{MPI commands and UNIX systems calls in ParGAP/MPI}

This chapter can be safely ignored on a first reading, and maybe
permanently.  It is for application programmers who wish to develop
their own low-level message-based parallel application.  The
additional UNIX system calls in ParGAP/MPI may also be useful in some
applications.

%======================================================================
\Section{Tutorial introduction to the MPI C library}

\index{MPI}\index{Message Passing Interface}
\index{MPI model}
\index{tutorial!MPI}

This section lists some of the more common message passing commands,
followed by a short MPI example.  The next section ("Other low level
commands") contains more (but by no means all) of the MPI commands and
some UNIX system calls.  The ParGAP/MPI binding provides a simplified form that
makes interactive usage easier.  This section describes the original MPI
binding in~C, with some comments about the interactive versions
provided in ParGAP/MPI.  (The MPI standard includes a binding both
to~C and to~FORTRAN.)

Even if your ultimate goal is a standalone C-based application, it is
useful to prototype your application with equivalent commands executed
interactively within ParGAP/MPI.  Note that this distribution
includes a subdirectory `.../pkg/pargap/mpinu/', which
provides a subset MPI implementation in~C with a small <footprint>.  It
consists of a C~library, `libmpi.a', and an include file `mpi.h'.  The
library is approximately 150~KB.  The subdirectory can be consulted
for further details.

We first briefly explain some MPI concepts.

\beginitems

*rank*:& The <rank> of an MPI process is a unique ID associated with
the process.  By convention, the console (master) process has rank~0.
The ranks of the process are guaranteed by MPI to form a consecutive,
ascending sequence of integers, starting with~0.

*tag*:& Each message has associated with it a non-negative integer
<tag> specified by the application.  Our interface allows you to
ignore tags by letting them take on default values.  Typical
application uses for tags are either to choose consecutive integers in
order to guarantee that all messages can be re-assembled in sequence,
or to choose a fixed set of constant tags, each constant associated
with another type of message.  In the latter case, one might have
integers for a `QUIT_TAG', an `INTEGER_ARRAY_TAG', `START_TASK2_TAG',
etc.  In fact, our implementation of the <slave listener> and
`MasterSlave()' specifically uses certain tags of value 1000 and
higher for these purposes.  Hence, application routines that do use
tags should restrict themselves to tags `[0..999]'.

*communicator*:& A <communicator> in MPI serves the purpose of a
namespace.  Most MPI commands require a communicator argument to
specify the namespace.  MPI starts up with a default namespace,
`MPI_COMM_WORLD'.  The ParGAPMPI implementation always assumes that
single namespace.  A namespace is important in MPI to build modules
and library routines, so that a thread may distinguish messages meant
for itself, or to catch errors of cross-communication between two modules.

*message*:& Each message in MPI is typically implemented to include
fields for the source rank, destination rank (optional), tag,
communicator, count, and an array of data.  The <count> field
specifies the length of the array.  MPI guarantees that messages are
non-overtaking, in the sense that if two messages are sent from a single
source process to the same destination process, then it is guaranteed
that the first process sent will be the first one to arrive, and will
be received or probed first from the queue.

*other*: MPI also has concepts of <datatype>, <derived datatype>,
<group>, <topology>, etc.  This implementation defaults those values,
so that <datatype> is always a character (hence the use of strings in
ParGAP/MPI), no <derived datatypes> are implemented, <group> is always
consistent with `MPI_COMM_WORLD', and <topology> is the fully
connected topology.

*communication*:  This implementation implements only point-to-point
communication (always blocking receives, except for `MPI_Iprobe', and
sends can be blocking or not, according to the default underlying
sockets).

*collective communication*:& The MPI standard also provides for
<collective communication>, which sets up a barrier in which all
process within the named communicator must participate.  One process
is distinguished as the <root> procees in cases of asymmetric usage.
ParGAP/MPI does not implement any collective communication (although
you can easily emulate it using a sequence of point-to-point
commands).  The MPI subset distribution (`.../pkg/pargap/mpinu')
does provide some commands for collective communication.  Examples of
MPI collective communication commands are `MPI_Bcast' (broadcast),
`MPI_Gather' (place an entry from each process in an array residing on
the root process), `MPI_Scatter' (inverse of gather), `MPI_Reduce'
(execute a commutative, associative function with an entry from each
process and store on root; example functions are <sum>, <and>, <xor>,
etc.

*dynamic processes*:& The newer MPI-2 standard allows for the dynamic
creation of new processes on new processors in an ongoing MPI
computation.  The standard is silent on whether an MPI session should
be aborted if one of its member processes dies, and the MPI standard
provides no mechanism to recognize such a dead process.  Part of the
reason for this silence is that much of the ancestry of MPI lies in
dedicated parallel computers for which it would be unusual for one process
or processor to die.
\enditems

Here is a short extract of MPI code to illustrate its flavor.  It
illustrates the C equivalents of the following ParGAP/MPI commands.
Note that the ParGAP/MPI versions noted here take fewer parameters
than their C-based cousins, and ParGAP/MPI includes defaults for some
optional parameters.

{\catcode 95=12
\>MPI_Init()!{example} %
              [ called for you automatically when ParGAP/MPI is loaded ] F
\>MPI_Finalize()!{example} [ called for you automatically when GAP quits ] F
\>MPI_Comm_rank()!{example} F
\>MPI_Get_count()!{example} F
\>MPI_Get_source()!{example} F
\>MPI_Get_tag()!{example} F
\>MPI_Comm_size()!{example} F
\>MPI_Send( <string buf>, <int dest>[, <opt int tag = 0> ] )!{example} F
\>MPI_Recv( <string buf>, <opt int source = MPI_ANY_SOURCE>[, %
            <opt int tag = MPI_ANY_TAG> ] )!{example} F
\>MPI_Probe( <opt int source = MPI_ANY_SOURCE>[, %
             <opt int tag = MPI_ANY_TAG> ] )!{example} F
\par}

Many of the above commands have analogues at a higher level in
section~"Slave Listener Commands" as `GetLastMsgSource()',
`GetLastMsgTag()', `MPI_Comm_size() = TOPCnumSlaves + 1', `SendMsg()',
`RecvMsg()' and `ProbeMsg()'.

\beginexample
#include <stdlib.h>
#include <mpi.h>

#define MYCOUNT 5
#define INT_TAG 1

main( int argc, char *argv[] )
{
  int myrank;
  MPI_Init( &argc, &argv );
  MPI_Comm_rank( MPI_COMM_WORLD, &myrank );

  if ( myrank == 0 ) {
    int mysize, dest, i;
    int buf;
    printf("My rank (master):  %d\n", myrank);
    for ( i=0; i<MYCOUNT; i++ )
      buf = 5;
    MPI_Comm_size( MPI_COMM_WORLD, &mysize );
    printf("Size:  %d\n", mysize);
    for ( dest=1; dest< mysize; dest++ )
      MPI_Send( &buf, MYCOUNT, MPI_INT, dest, INT_TAG, MPI_COMM_WORLD );
  } else {
    int i;
    MPI_Status status;
    int source;
    int count;
    int *buf;
    printf("My rank (slave):  %d\n", myrank);

    MPI_Probe( MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status );
    printf( "Message pending with rank %d and tag %d.\n",
            status.MPI_SOURCE, status.MPI_TAG );
    if ( status.MPI_TAG != INT_TAG )
      printf("Error: Bad tag.\n"); exit(1);
    MPI_Get_count( &status, MPI_INT, &count );
    printf( "The count of how many data units (MPI_INT) is:  %d.\n", count );
    buf = (int *)malloc( count * sizeof(int) );

    source = status.MPI_SOURCE;
    MPI_Recv( buf, count, MPI_INT, source, INT_TAG, MPI_COMM_WORLD, &status );
    for ( i=0; i<MYCOUNT; i++ )
      if ( *buf != 5 ) printf("error:  buf[%d] != 5\n", i);
    printf("slave %d done.\n", myrank);
    }
  MPI_Finalize();
  exit(0);
}
\endexample

Even in this simplistic example, it was important to specify
\begintt
    MPI_Recv( buf, count, MPI_INT, source, INT_TAG, MPI_COMM_WORLD, &status );
\endtt
and not to use `MPI_ANY_SOURCE' instead of the known `source'.  Although this alternative would
often work, there is a danger that there might be a second incoming
message from a different source that arrives between the calls to
`MPI_Probe()' and `MPI_Recv()'.  In such an event, MPI would be free
to receive the second message in `MPI_Recv()', even though the
appropriate count of the second message is likely to be different,
thus risking an overflow of the`buf' buffer.

Other typical bugs in MPI programs are:
\beginitems
+& Incorrectly matching corresponding sends and receives or having
more or fewer sends than receives due to the logic of multiple sends
and receives within distinct loops.

+& Reaching deadlock because all active processes have blocking
calls to `MPI_Recv()' while no process has yet reached code that
executes `MPI_Send()'.

+& Incorrect use of barriers in collective communication, whereby one
process might execute:
\begintt
MPI_Send( buf, count, datatype, dest, tag, COMM_1 );
MPI_Bcast( buffer, count, datatype, root, COMM_2 );
\endtt
and a second executes
\begintt
MPI_Bcast( buffer, count, datatype, root, COMM_2 );
MPI_Recv( buf, count, datatype, dest, tag, COMM_1, status );
\endtt
If the call to `MPI_Send()' is blocking (as is the case for long
messages in the case of many implementations), then the first process
will block at `MPI_Send()' while the second blocks at 'MPI_Bcast()'.
This happens even though they use distinct communicators, and the
send-receive communication would not normally interact with the
broadcast communication.
\enditems

Much of the TOP-C method in ParGAP/MPI (see chapters~"Basic Concepts
for the TOP-C model (MasterSlave)" and~"Tutorial using TOP-C and
MasterSlave()") was developed precisely to make errors like those
above syntactically impossible.  The slave listener layer also does some
additional work to keep track of the `status' that was last received
and other bookkeeping.  Additionally, the TOP-C method was
designed to provide a higher level, task-oriented ``language'', which
would naturally lead the application programmer into designing an
efficient high level algorithm.

%======================================================================
\Section{Other low level commands}

\index{MPI commands!All ParGAP/MPI bindings}
\index{UNIX system calls!All ParGAP/MPI bindings}

Here is a complete listing of the low level commands available in
ParGAP/MPI.  Some of these commands were documented elsewhere.  The
remaining ones are not recommended for most users.  Nevertheless, it
may be useful to others for more sophisticated applications.

For most of these commands, the source code is the ultimat documentation.
However, you may be able to guess at the meaning of many of them based on
their names and their similarity UNIX system calls (in the case of
`UNIX_'...) or MPI commands (in the case of `MPI'...).  Some of the
commands will also show you their calling parameters if called with
the wrong number of arguments.  Many of the
MPI commands have simplified calling parameters with certain
arguments optional or set to defaults, making them easier for interactive use.

\index{UNIX functions}%
\index{functions!UNIX}

{\catcode 95=12
\>UNIX_MakeString( <len> ) F
\>UNIX_DirectoryCurrent() [ Defined in `pkg/pargap/lib/slavelist.g' ] F
\>UNIX_Chdir( <string> ) F
\>UNIX_FflushStdout() F
\>UNIX_Catch( <function>, <return_val> ) F
\>UNIX_Throw() F
\>UNIX_Getpid() F
\>UNIX_Hostname() F
\>UNIX_Alarm( <seconds> ) F
\>UNIX_Realtime() F
\>UNIX_Nice( <priority> ) F
\>UNIX_LimitRss( <bytes_of_ram> ) [ = setrlimit(RLIMIT_RSS, ...) ] F

\index{MPI functions}%
\index{functions!MPI}

\>MPI_Init() F
\>MPI_Initialized() F
\>MPI_Finalize() F
\>MPI_Comm_rank() F
\>MPI_Get_count() F
\>MPI_Get_source() F
\>MPI_Get_tag() F
\>MPI_Comm_size() F
\>MPI_World_size() F
\>MPI_Error_string( <errorcode> ) F
\>MPI_Get_processor_name() F
\>MPI_Attr_get( <keyval> ) F
\>MPI_Abort( <errorcode> ) F
\>MPI_Send( <string buf>, <int dest>[, <opt int tag = 0> ] ) F
\>MPI_Recv( <string buf>, <opt int source = MPI_ANY_SOURCE>[, <opt int tag = MPI_ANY_TAG> ] ) F
\>MPI_Probe( <opt int source = MPI_ANY_SOURCE>[, <opt int tag = MPI_ANY_TAG> ] ) F
\>MPI_Iprobe( <opt int source = MPI_ANY_SOURCE>[, <opt int tag = MPI_ANY_TAG> ] ) F

\index{MPI global constants}%
\index{constants!MPI, global}

\>MPI_ANY_SOURCE V
\>MPI_ANY_TAG V
\>MPI_COMM_WORLD V
\>MPI_TAG_UB V
\>MPI_HOST V
\>MPI_IO V
\par}


% =========================================================================
\Chapter{Comments?}

COMMENTS SOLICITED:

I welcome comments on how well the TOP-C parallel model fits other
applications.  I am also interested in building up a library of
ParGAP/MPI programs that can be made available to other users.
Finally, one ingredient in making a system usabe is a good choice of
names that makes the purpose of various commands obvious to a new
user.  I welcome suggestions.  For example, ParEval, BroadcastMsg,
SendRecvMsg, and SendMsg all have related functionalities.  The same
can be said of RecvMsg, ProbeMsg, ProbeMsgNonBlocking Hence, a more
orthogonal naming scheme might be easier.

If you are interested in ``looking over my shoulder'', you might also
want to inspect some of my random scribbling in the `pargap/etc'
subdirectory.


% \begintt
% TODO:
% 1) Can talk a little about CONTINUATION_ACTION(), RawMasterSlave() and
%     futures and shared memory,
%    and multiple environments, and dynamically joining processes, Beowulf, etc.
% 2)  malloc issues (okay?)
% 3)  "<string_buf>;_<int_dest>[;_<opt_int_tag_=_0>_]"
%    Make some things immutable, masslave.g, and MPI_TAG_UB, etc. in pargap.c?
% 4)  MPI_Type_size(), tags in any order (pool of buffers)
% 5)  ^C or InterruptSlave(1) gives weird error:
%   Maybe this will go away in GAP 4.x, which allows empty statements.
% Syntax error: expression expected in stream line 1
%  y^
% This message should be ignored.  If on the master inside a "brk>" loop,
%  type "quit;" to leave the break loop.
% \endtt
